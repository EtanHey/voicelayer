{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"VoiceLayer","text":"<p>Voice I/O layer for AI coding assistants \u2014 local TTS, STT, session booking.</p> <p>VoiceLayer adds voice input and output to Claude Code sessions via the Model Context Protocol (MCP). Speak questions aloud, record voice responses, and transcribe locally with whisper.cpp \u2014 all inside your terminal.</p>"},{"location":"#why-voicelayer","title":"Why VoiceLayer?","text":"<p>AI coding assistants are text-only. But some tasks are faster with voice:</p> <ul> <li>QA testing \u2014 browse a page, speak what you see, let the agent take notes</li> <li>Discovery calls \u2014 hands-free client interviews with automatic briefs</li> <li>Code review \u2014 explain your reasoning while the agent captures it</li> <li>Drilling sessions \u2014 interactive Q&amp;A with voice responses</li> </ul> <p>VoiceLayer bridges the gap between your terminal and your microphone.</p>"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code>Claude Code  \u2500\u2500\u2500 MCP \u2500\u2500\u2500&gt;  VoiceLayer\n                            \u251c\u2500\u2500 Waits for any playing voice_speak audio\n                            \u251c\u2500\u2500 edge-tts speaks question (speakers)\n                            \u251c\u2500\u2500 sox records mic (native rate \u2192 resample to 16kHz)\n                            \u251c\u2500\u2500 Silero VAD detects speech/silence\n                            \u251c\u2500\u2500 whisper.cpp transcribes locally (~300ms)\n                            \u2514\u2500\u2500 Returns transcription to Claude\n</code></pre> <ol> <li>Claude calls <code>voice_ask(\"How does the nav look on mobile?\")</code></li> <li>VoiceLayer waits for any prior <code>voice_speak</code> audio to finish (no overlap)</li> <li>Speaks the question aloud via edge-tts</li> <li>Mic recording starts at device's native sample rate (auto-detected)</li> <li>Audio resampled to 16kHz in real-time, fed to Silero VAD for speech detection</li> <li>Recording ends on user stop signal, VAD silence detection, or timeout</li> <li>Audio transcribed by whisper.cpp (local) or Wispr Flow (cloud fallback)</li> <li>Claude receives the transcribed text and continues</li> </ol>"},{"location":"#voice-tools","title":"Voice Tools","text":"Tool What It Does Blocking voice_speak Non-blocking TTS \u2014 auto-selects announce/brief/consult/think from message No voice_ask Blocking voice Q&amp;A \u2014 speak question, record + transcribe response Yes <p>Mode-specific guidance: Announce, Brief, Consult, Converse, Think. Old <code>qa_voice_*</code> names still work as backward-compat aliases.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>100% local STT \u2014 whisper.cpp on Apple Silicon, no cloud dependency</li> <li>Session booking \u2014 lockfile mutex prevents mic conflicts between sessions</li> <li>User-controlled stop \u2014 <code>touch /tmp/voicelayer-stop</code>, or Silero VAD silence detection (quick 0.5s, standard 1.5s, thoughtful 2.5s)</li> <li>Per-mode speech rates \u2014 announce is snappy (+10%), brief is slow (-10%)</li> <li>Auto-slowdown \u2014 long text automatically gets slower speech rate</li> <li>Cross-platform \u2014 macOS and Linux support</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>brew install sox whisper-cpp\npip3 install edge-tts\n</code></pre> <p>Then add to your <code>.mcp.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"voicelayer\": {\n      \"command\": \"bunx\",\n      \"args\": [\"voicelayer-mcp\"]\n    }\n  }\n}\n</code></pre> <p>See the full Quick Start guide for details, or read What is VoiceLayer? for a non-technical overview.</p>"},{"location":"#platform-support","title":"Platform Support","text":"Platform TTS Audio Player STT Recording macOS edge-tts afplay (built-in) whisper.cpp sox/rec Linux edge-tts mpv, ffplay, or mpg123 whisper.cpp sox/rec"},{"location":"contributing/","title":"Contributing","text":"<p>Thanks for considering contributing to VoiceLayer! This guide covers development setup, testing, and the PR process.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code># Clone the repo\ngit clone https://github.com/EtanHey/voicelayer.git\ncd voicelayer\n\n# Install dependencies\nbun install\n\n# Install audio prerequisites (macOS)\nbrew install sox whisper-cpp\npip3 install edge-tts\n\n# Download a whisper model for STT tests\nmkdir -p ~/.cache/whisper\ncurl -L -o ~/.cache/whisper/ggml-base.en.bin \\\n  https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>voicelayer/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 mcp-server.ts          # MCP server entry point\n\u2502   \u251c\u2500\u2500 tts.ts                 # Text-to-speech (edge-tts)\n\u2502   \u251c\u2500\u2500 input.ts               # Mic recording (sox)\n\u2502   \u251c\u2500\u2500 stt.ts                 # STT backends (whisper.cpp + Wispr)\n\u2502   \u251c\u2500\u2500 audio-utils.ts         # Shared audio utilities\n\u2502   \u251c\u2500\u2500 paths.ts               # /tmp path constants\n\u2502   \u251c\u2500\u2500 session-booking.ts     # Lockfile-based session mutex\n\u2502   \u251c\u2500\u2500 session.ts             # Session lifecycle\n\u2502   \u251c\u2500\u2500 report.ts              # QA report renderer\n\u2502   \u251c\u2500\u2500 brief.ts               # Discovery brief renderer\n\u2502   \u251c\u2500\u2500 schemas/               # QA + discovery category schemas\n\u2502   \u2514\u2500\u2500 __tests__/             # Test files\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 speak.sh               # Standalone TTS command\n\u2502   \u2514\u2500\u2500 test-wispr-ws.ts       # Wispr Flow WebSocket test\n\u251c\u2500\u2500 flow-bar/                  # SwiftUI macOS Voice Bar app\n\u251c\u2500\u2500 docs/                      # MkDocs documentation (this site)\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 tsconfig.json\n\u251c\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nbun test\n\n# Run a specific test file\nbun test src/__tests__/tts.test.ts\n\n# Run tests matching a pattern\nbun test --grep \"session booking\"\n</code></pre> <p>The test suite covers 230 tests with 463 assertions across:</p> <ul> <li>TTS synthesis and playback</li> <li>STT backend detection and transcription</li> <li>Session booking (lock/release/stale cleanup)</li> <li>Input recording and silence detection</li> <li>QA report and discovery brief rendering</li> <li>MCP server tool routing</li> </ul> <p>Most tests mock external tools (sox, edge-tts, whisper-cpp) so they run without audio hardware.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>TypeScript with strict mode</li> <li>Bun runtime (not Node.js)</li> <li>No formatting tools \u2014 keep consistent with existing code</li> <li>JSDoc comments on exported functions</li> <li>Module header comments describing purpose and dependencies</li> </ul>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#branch-from-main","title":"Branch from main","text":"<pre><code>git checkout main\ngit pull origin main\ngit checkout -b feature/your-feature\n</code></pre>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits:</p> <pre><code>feat: add new voice mode\nfix: handle edge-tts timeout\ndocs: update STT backend comparison\ntest: add session booking race condition test\nchore: update dependencies\n</code></pre>"},{"location":"contributing/#submit-a-pr","title":"Submit a PR","text":"<ol> <li>Push your branch: <code>git push -u origin feature/your-feature</code></li> <li>Create a PR against <code>main</code></li> <li>CodeRabbit will auto-review within 1-2 minutes</li> <li>Address any HIGH severity findings</li> <li>Merge when clean</li> </ol>"},{"location":"contributing/#architecture-principles","title":"Architecture Principles","text":"<ul> <li>Delegate audio I/O \u2014 use system tools (sox, edge-tts, whisper-cpp), don't bundle audio libraries</li> <li>File-based IPC \u2014 lockfiles and touch-files for cross-process communication</li> <li>Fail gracefully \u2014 never throw from MCP tool handlers, always return structured errors</li> <li>Local-first \u2014 prefer local processing (whisper.cpp) over cloud services</li> <li>Zero config \u2014 sensible defaults, auto-detection for models and backends</li> </ul>"},{"location":"contributing/#adding-a-new-voice-mode","title":"Adding a New Voice Mode","text":"<ol> <li>Define the tool in <code>mcp-server.ts</code> (ListToolsRequestSchema handler)</li> <li>Add a handler function (<code>handleYourMode</code>)</li> <li>Route it in the CallToolRequestSchema switch</li> <li>Add tests in <code>src/__tests__/</code></li> <li>Document in <code>docs/modes/your-mode.md</code></li> <li>Add to the nav in <code>mkdocs.yml</code></li> </ol>"},{"location":"contributing/#adding-a-new-stt-backend","title":"Adding a New STT Backend","text":"<ol> <li>Implement the <code>STTBackend</code> interface in <code>stt.ts</code>:    <pre><code>interface STTBackend {\n  name: string;\n  isAvailable(): Promise&lt;boolean&gt;;\n  transcribe(audioPath: string): Promise&lt;STTResult&gt;;\n}\n</code></pre></li> <li>Add detection logic in <code>getBackend()</code></li> <li>Add tests for availability detection and transcription</li> <li>Document in <code>docs/architecture/stt-backends.md</code></li> </ol>"},{"location":"tools-reference/","title":"MCP Tools Reference","text":"<p>VoiceLayer exposes 2 primary tools. Backward-compat aliases are listed below.</p>"},{"location":"tools-reference/#voice_speak","title":"voice_speak","text":"<p>Non-blocking text-to-speech. Speaks a message aloud or logs it silently. Auto-detects mode from message content if <code>mode</code> is omitted.</p> Property Value Blocking No Requires mic No Session booking No <p>Parameters:</p> Name Type Required Default Description <code>message</code> <code>string</code> Yes \u2014 Text to speak or log (must be non-empty after trimming) <code>mode</code> <code>string</code> No <code>auto</code> <code>announce</code>, <code>brief</code>, <code>consult</code>, <code>think</code>, or <code>auto</code> (auto-detect from content) <code>voice</code> <code>string</code> No <code>jenny</code> Profile name or raw edge-tts voice ID <code>rate</code> <code>string</code> No (per-mode) Speech rate (e.g., <code>-10%</code>, <code>+5%</code>). Pattern: <code>^[+-]\\d+%$</code> <code>category</code> <code>string</code> No <code>insight</code> For think mode: <code>insight</code>, <code>question</code>, <code>red-flag</code>, <code>checklist-update</code> <code>replay_index</code> <code>number</code> No \u2014 Replay cached message (0 = most recent). Ignores message. <code>enabled</code> <code>boolean</code> No \u2014 Toggle voice on/off instead of speaking <code>scope</code> <code>string</code> No <code>all</code> Toggle scope: <code>all</code>, <code>tts</code>, or <code>mic</code> (only with <code>enabled</code>) <p>Mode auto-detection: <code>insight:</code>, <code>note:</code>, <code>TODO:</code> \u2192 think; <code>?</code> or \"about to\" \u2192 consult; &gt;280 chars \u2192 brief; default \u2192 announce.</p> <p>Returns: <code>[mode] Spoke: \"message\"</code> or <code>Noted (category): thought</code> for think mode. Errors: Empty message, edge-tts not installed, audio player missing</p>"},{"location":"tools-reference/#voice_ask","title":"voice_ask","text":"<p>Blocking voice Q&amp;A. Auto-waits for any playing <code>voice_speak</code> audio to finish, then speaks a question aloud, records mic at device's native rate (auto-detected), resamples to 16kHz, transcribes via Silero VAD + whisper.cpp/Wispr Flow, returns text.</p> Property Value Blocking Yes Requires mic Yes Session booking Yes (auto-books on first call) Auto-waits Yes (waits for prior <code>voice_speak</code> playback) <p>Parameters:</p> Name Type Required Default Description <code>message</code> <code>string</code> Yes \u2014 Question to speak aloud (must be non-empty) <code>timeout_seconds</code> <code>number</code> No <code>300</code> Max wait time (clamped to 10-3600) <code>silence_mode</code> <code>string</code> No <code>thoughtful</code> <code>quick</code>, <code>standard</code>, or <code>thoughtful</code> <p>Returns (success): The user's transcribed text (plain string) Returns (timeout): <code>[converse] No response received within N seconds.</code> Returns (busy): <code>[converse] Line is busy \u2014 voice session owned by...</code> (with <code>isError: true</code>)</p> <p>Errors:</p> Error Cause Line busy Another session has the mic sox not installed <code>rec</code> command missing Mic permission denied Terminal not authorized for mic No STT backend Neither whisper.cpp nor Wispr available"},{"location":"tools-reference/#backward-compat-aliases","title":"Backward-Compat Aliases","text":"Alias Maps To <code>qa_voice_announce</code> <code>voice_speak(mode='announce')</code> <code>qa_voice_brief</code> <code>voice_speak(mode='brief')</code> <code>qa_voice_consult</code> <code>voice_speak(mode='consult')</code> <code>qa_voice_think</code> <code>voice_speak(mode='think')</code> (uses <code>thought</code> param) <code>qa_voice_say</code> <code>voice_speak(mode='announce')</code> <code>qa_voice_replay</code> <code>voice_speak(replay_index=N)</code> <code>qa_voice_toggle</code> <code>voice_speak(enabled=bool)</code> <code>qa_voice_converse</code> <code>voice_ask</code> <code>qa_voice_ask</code> <code>voice_ask</code>"},{"location":"tools-reference/#error-handling","title":"Error Handling","text":"<p>All tools return errors in MCP format:</p> <pre><code>{\n  \"content\": [{ \"type\": \"text\", \"text\": \"Error message here\" }],\n  \"isError\": true\n}\n</code></pre> <p>Tools never throw exceptions \u2014 all errors are caught and returned as structured responses. Errors are also logged to stderr for debugging.</p>"},{"location":"tools-reference/#prerequisites-summary","title":"Prerequisites Summary","text":"Tool Depends On voice_speak (TTS modes) <code>python3</code> + <code>edge-tts</code>, audio player voice_ask All of the above + <code>sox</code>, STT backend (whisper.cpp or Wispr) voice_speak (think mode) None (file system only)"},{"location":"what-is-voicelayer/","title":"What is VoiceLayer?","text":"<p>VoiceLayer lets Claude talk to you and listen.</p> <p>When you use Claude Code, everything happens through text \u2014 you type, Claude types back. VoiceLayer adds voice to that conversation.</p>"},{"location":"what-is-voicelayer/#what-can-it-do","title":"What Can It Do?","text":"<ul> <li>Claude speaks to you through your speakers \u2014 status updates, explanations, questions</li> <li>You speak back through your microphone \u2014 Claude transcribes what you said and continues working</li> <li>Everything stays local \u2014 your voice is transcribed on your machine, nothing leaves your computer</li> </ul>"},{"location":"what-is-voicelayer/#real-examples","title":"Real Examples","text":""},{"location":"what-is-voicelayer/#qa-testing-a-website","title":"QA Testing a Website","text":"<p>You're reviewing a client's website with Claude. Instead of typing descriptions of what you see:</p> <p>Claude (voice): \"How does the checkout page look on mobile?\"</p> <p>You (voice): \"The payment form is cut off on the right. And the submit button is hidden behind the keyboard.\"</p> <p>Claude records this, moves to the next page, and keeps testing.</p>"},{"location":"what-is-voicelayer/#hands-free-code-review","title":"Hands-Free Code Review","text":"<p>You're walking through code changes while Claude takes notes:</p> <p>Claude (voice): \"I found three changes in the auth module. Want me to walk through them?\"</p> <p>You (voice): \"Yes, start with the middleware changes.\"</p>"},{"location":"what-is-voicelayer/#background-notifications","title":"Background Notifications","text":"<p>Claude finishes a long task while you're reading docs in another window:</p> <p>Claude (voice): \"Build complete. 47 tests passing, 2 skipped.\"</p> <p>No need to switch back to the terminal to check.</p>"},{"location":"what-is-voicelayer/#how-it-works-simply","title":"How It Works (Simply)","text":"<ol> <li>You add VoiceLayer to Claude Code (one line in a config file)</li> <li>Claude gains 5 new voice tools \u2014 announce, brief, consult, converse, and think</li> <li>When Claude wants to speak, it calls the voice tool</li> <li>When Claude needs your input, it speaks a question, records your answer, and reads the transcription</li> </ol> <p>The entire flow happens in your terminal. No browser, no app, no account needed.</p>"},{"location":"what-is-voicelayer/#what-you-need","title":"What You Need","text":"<ul> <li>A Mac or Linux computer with speakers and a microphone</li> <li>Claude Code installed</li> <li>Bun (a JavaScript runtime \u2014 one command to install)</li> <li>sox (for microphone recording \u2014 one command to install)</li> <li>edge-tts (for text-to-speech \u2014 one command to install)</li> <li>whisper.cpp (for speech-to-text \u2014 optional but recommended for fully local operation)</li> </ul> <p>Total setup time: about 5 minutes. See the Quick Start guide.</p>"},{"location":"what-is-voicelayer/#is-it-free","title":"Is It Free?","text":"<p>Yes. VoiceLayer is open source (MIT license). All voice processing runs locally on your machine. The only optional cloud component is Wispr Flow for speech-to-text, which requires an API key \u2014 but the default whisper.cpp backend is fully local and free.</p>"},{"location":"advanced/voice-bar/","title":"Voice Bar","text":"<p>The Voice Bar is a floating macOS widget that provides visual feedback and quick controls for VoiceLayer. Built with SwiftUI, it connects to the MCP server via Unix socket IPC.</p>"},{"location":"advanced/voice-bar/#requirements","title":"Requirements","text":"<ul> <li>macOS 14+ (Sonoma or later)</li> <li>Swift toolchain (Xcode or <code>swift</code> CLI)</li> <li>VoiceLayer MCP server running</li> </ul>"},{"location":"advanced/voice-bar/#quick-start","title":"Quick Start","text":"<pre><code># Build and launch\nvoicelayer bar\n\n# Stop\nvoicelayer bar-stop\n</code></pre> <p>The Voice Bar appears as a floating pill at the top of your screen.</p>"},{"location":"advanced/voice-bar/#features","title":"Features","text":""},{"location":"advanced/voice-bar/#state-display","title":"State Display","text":"<p>The pill changes appearance based on VoiceLayer state:</p> State Appearance Idle Collapses to a small dot after 5s of inactivity Speaking Expanded pill with teleprompter showing spoken text Recording Pulsing waveform visualization driven by audio levels Transcribing Loading indicator while whisper.cpp processes Error Red indicator with error message"},{"location":"advanced/voice-bar/#teleprompter","title":"Teleprompter","text":"<p>During <code>voice_speak</code>, the pill expands to show the text being spoken:</p> <ul> <li>Short text (8 words or fewer) renders centered</li> <li>Long text uses a scrolling view with automatic scroll-to-current</li> <li>Wider reading area (220pt, 11pt font) for comfortable reading</li> <li>Punctuation-aware timing \u2014 pauses at periods and commas</li> </ul>"},{"location":"advanced/voice-bar/#audio-waveform","title":"Audio Waveform","text":"<p>During recording, a real-time waveform visualization shows microphone levels:</p> <ul> <li>RMS audio levels received every ~100ms via socket events</li> <li>Bar heights animated with spring physics</li> <li>Visual confirmation that the mic is picking up audio</li> </ul>"},{"location":"advanced/voice-bar/#click-to-record","title":"Click-to-Record","text":"<p>Tap the pill to start recording. When recording stops, the transcribed text is automatically pasted into the active application via <code>Cmd+V</code> (CGEvent).</p>"},{"location":"advanced/voice-bar/#draggable-positioning","title":"Draggable Positioning","text":"<ul> <li>Drag the pill anywhere on screen</li> <li>Position saved as screen percentage (survives resolution changes)</li> <li>Restored on launch and when switching monitors</li> </ul>"},{"location":"advanced/voice-bar/#idle-collapse","title":"Idle Collapse","text":"<p>After 5 seconds of idle state, the pill collapses to a minimal dot. It expands automatically when any voice activity starts, or on hover.</p>"},{"location":"advanced/voice-bar/#architecture","title":"Architecture","text":"<pre><code>VoiceLayer MCP Server\n    \u2502\n    \u2502  /tmp/voicelayer.sock (NDJSON)\n    \u2502\n    \u25bc\nVoice Bar (SwiftUI)\n    \u251c\u2500\u2500 NWConnection (Unix socket client)\n    \u251c\u2500\u2500 @Observable VoiceLayerState\n    \u251c\u2500\u2500 PillView (main UI)\n    \u251c\u2500\u2500 TeleprompterView (text display)\n    \u2514\u2500\u2500 WaveformView (audio levels)\n</code></pre>"},{"location":"advanced/voice-bar/#socket-protocol","title":"Socket Protocol","text":"<p>The Voice Bar communicates via NDJSON events over <code>/tmp/voicelayer.sock</code>:</p> <p>Events (server \u2192 client):</p> Event Fields Description <code>state</code> <code>state</code>, <code>text?</code> State change (idle, speaking, recording, transcribing, error) <code>speech</code> <code>text</code>, <code>index</code> Text being spoken <code>audio_level</code> <code>level</code> RMS 0.0-1.0, every ~100ms during recording <code>transcription</code> <code>text</code> Transcription result <code>error</code> <code>message</code> Error message <p>Commands (client \u2192 server):</p> Command Description <code>stop</code> Stop current playback or recording <code>cancel</code> Cancel current operation <code>replay</code> Replay last TTS output <code>toggle</code> Toggle TTS/mic on/off <code>record</code> Start recording"},{"location":"advanced/voice-bar/#building-from-source","title":"Building from Source","text":"<pre><code>cd flow-bar\nswift build -c release\n.build/release/FlowBar\n</code></pre> <p>The Voice Bar uses <code>NSPanel</code> with <code>.nonactivatingPanel</code> style so it never steals focus from your editor.</p>"},{"location":"advanced/voice-cloning/","title":"Voice Cloning","text":"<p>VoiceLayer supports zero-shot voice cloning using Qwen3-TTS. Clone any voice from YouTube samples \u2014 no model training required.</p>"},{"location":"advanced/voice-cloning/#how-it-works","title":"How It Works","text":"<pre><code>YouTube URL \u2192 yt-dlp (WAV 48kHz) \u2192 Silero VAD segmentation \u2192 FFmpeg normalization\n\u2192 voicelayer clone (select best 3 clips) \u2192 profile.yaml \u2192 Qwen3-TTS daemon\n</code></pre> <p>Three-tier TTS routing at runtime:</p> <ol> <li>Qwen3-TTS \u2014 cloned voice (if daemon running + profile exists)</li> <li>edge-tts \u2014 Microsoft neural voice (free, always available)</li> <li>Text-only \u2014 fallback when no audio output possible</li> </ol>"},{"location":"advanced/voice-cloning/#prerequisites","title":"Prerequisites","text":"<pre><code># Required\nbrew install yt-dlp ffmpeg\npip3 install silero-vad torch soundfile\n\n# Optional (better quality)\npip3 install demucs            # Source separation (removes background music)\npip3 install pyannote.audio    # Speaker diarization (multi-speaker videos)\n\n# TTS daemon\npip3 install mlx-audio fastapi uvicorn\n</code></pre>"},{"location":"advanced/voice-cloning/#step-1-extract-voice-samples","title":"Step 1: Extract Voice Samples","text":"<p>Pull clean voice clips from YouTube videos:</p> <pre><code>voicelayer extract \\\n  --source \"https://youtube.com/@channel\" \\\n  --name \"speaker-name\" \\\n  --count 20\n</code></pre> <p>The extraction pipeline:</p> <ol> <li>Downloads audio via <code>yt-dlp</code> (WAV 48kHz)</li> <li>Optionally runs Demucs source separation (<code>--demucs</code>)</li> <li>Segments speech using Silero VAD</li> <li>Optionally diarizes speakers (<code>--diarize</code>)</li> <li>Normalizes audio (highpass 80Hz, loudnorm -16 LUFS)</li> <li>Saves clips to <code>~/.voicelayer/voices/{name}/samples/</code></li> </ol>"},{"location":"advanced/voice-cloning/#options","title":"Options","text":"Flag Default Description <code>--source</code> required YouTube URL (channel, video, or playlist) <code>--name</code> required Voice profile name <code>--count</code> <code>20</code> Number of clips to extract <code>--demucs</code> off Enable source separation <code>--diarize</code> off Enable speaker diarization"},{"location":"advanced/voice-cloning/#step-2-build-a-voice-profile","title":"Step 2: Build a Voice Profile","text":"<p>Select the best reference clips and generate a profile:</p> <pre><code>voicelayer clone --name \"speaker-name\"\n</code></pre> <p>This command:</p> <ol> <li>Reads all samples from <code>~/.voicelayer/voices/{name}/samples/</code></li> <li>Analyzes audio quality (duration, RMS level, SNR estimate)</li> <li>Selects the best 3 clips (~18.5s total \u2014 Qwen3-TTS sweet spot: 3-30s)</li> <li>Generates transcripts via whisper.cpp</li> <li>Writes <code>~/.voicelayer/voices/{name}/profile.yaml</code></li> </ol>"},{"location":"advanced/voice-cloning/#profile-format","title":"Profile Format","text":"<pre><code>name: speaker-name\nengine: qwen3-tts\nmodel_path: ~/.voicelayer/models/qwen3-tts-4bit\nreference_clips:\n  - path: ~/.voicelayer/voices/speaker-name/samples/clip_007.wav\n    text: \"Transcribed text of this clip...\"\n  - path: ~/.voicelayer/voices/speaker-name/samples/clip_012.wav\n    text: \"Another transcribed clip...\"\n  - path: ~/.voicelayer/voices/speaker-name/samples/clip_003.wav\n    text: \"Third reference clip...\"\nreference_clip: ~/.voicelayer/voices/speaker-name/samples/clip_007.wav\nfallback: en-US-GuyNeural\ncreated: \"2026-02-27\"\nsource: \"https://youtube.com/@channel\"\n</code></pre>"},{"location":"advanced/voice-cloning/#step-3-run-the-tts-daemon","title":"Step 3: Run the TTS Daemon","text":"<p>Start the Qwen3-TTS daemon for runtime synthesis:</p> <pre><code>voicelayer daemon --port 8880\n</code></pre> <p>The daemon:</p> <ul> <li>Loads the Qwen3-TTS 4-bit quantized model into Metal/MPS memory</li> <li>Serves a <code>/synthesize</code> HTTP endpoint</li> <li>Inference latency: 200-500ms per call on Apple Silicon</li> <li>Model location: <code>~/.voicelayer/models/qwen3-tts-4bit/</code></li> </ul>"},{"location":"advanced/voice-cloning/#testing","title":"Testing","text":"<pre><code># Health check\ncurl http://127.0.0.1:8880/health\n\n# Synthesize\ncurl -X POST http://127.0.0.1:8880/synthesize \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\": \"Hello world\", \"voice\": \"speaker-name\"}'\n</code></pre>"},{"location":"advanced/voice-cloning/#using-cloned-voices","title":"Using Cloned Voices","text":"<p>Once the daemon is running and a profile exists, VoiceLayer automatically routes through Qwen3-TTS:</p> <pre><code>voice_speak(\"Your cloned voice speaks this text\")\n</code></pre> <p>If the daemon is unavailable, VoiceLayer falls back to edge-tts using the <code>fallback</code> voice from the profile.</p>"},{"location":"advanced/voice-cloning/#file-locations","title":"File Locations","text":"Path Purpose <code>~/.voicelayer/voices/{name}/samples/</code> Extracted voice clips <code>~/.voicelayer/voices/{name}/profile.yaml</code> Voice profile <code>~/.voicelayer/models/qwen3-tts-4bit/</code> Quantized model weights"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>VoiceLayer is a lightweight MCP server that bridges Claude Code with your microphone and speakers. It's built with Bun and TypeScript, using system-level tools for audio I/O.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<pre><code>Claude Code Session\n    \u2502\n    \u2502  MCP (JSON-RPC over stdio)\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     VoiceLayer MCP Server  \u2502\n\u2502     (mcp-server.ts)        \u2502\n\u2502                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  TTS    \u2502  \u2502  Input  \u2502 \u2502\n\u2502  \u2502 (tts.ts)\u2502  \u2502(input.ts)\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502       \u2502            \u2502       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502edge-tts \u2502  \u2502   sox   \u2502 \u2502\n\u2502  \u2502(python3)\u2502  \u2502  (rec)  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502       \u2502            \u2502       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 afplay/ \u2502  \u2502   STT   \u2502 \u2502\n\u2502  \u2502  mpv    \u2502  \u2502(stt.ts) \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                    \u2502       \u2502\n\u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2510    \u2502\n\u2502          \u2502whisper.cpp \u2502    \u2502\n\u2502          \u2502or Wispr API\u2502    \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Session Booking     \u2502  \u2502\n\u2502  \u2502  (session-booking.ts)\u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#module-responsibilities","title":"Module Responsibilities","text":"Module File Responsibility MCP Server <code>mcp-server.ts</code> Tool definitions, request routing, argument validation TTS <code>tts.ts</code> Text-to-speech via edge-tts, audio playback, rate adjustment Input <code>input.ts</code> Mic recording via sox (native rate), Silero VAD speech detection, PCM resampling/WAV handling STT <code>stt.ts</code> Backend abstraction \u2014 whisper.cpp (local) or Wispr Flow (cloud) Session Booking <code>session-booking.ts</code> Lockfile mutex for mic access, stale lock cleanup Paths <code>paths.ts</code> Centralized <code>/tmp</code> path constants Audio Utils <code>audio-utils.ts</code> Shared audio utilities (RMS calculation, native rate detection, PCM resampling) Session <code>session.ts</code> Session lifecycle management (save/load/generate) Report <code>report.ts</code> QA report rendering (JSON -&gt; markdown) Brief <code>brief.ts</code> Discovery brief rendering (JSON -&gt; markdown) Schemas <code>schemas/</code> QA and discovery category/checklist definitions"},{"location":"architecture/overview/#data-flow-converse-mode","title":"Data Flow: Converse Mode","text":"<p>The most complex mode \u2014 full round-trip voice Q&amp;A:</p> <pre><code>1. Agent calls voice_ask(\"How does it look?\")\n                \u2502\n2. Session booking check (lockfile)\n                \u2502\n3. Wait for prior voice_speak playback to finish (if any)\n                \u2502\n4. edge-tts synthesizes \u2192 /tmp/voicelayer-tts-PID-N.mp3\n                \u2502\n5. afplay speaks the question (blocking \u2014 waits for completion)\n                \u2502\n6. Detect device native sample rate (e.g., 24kHz AirPods, 48kHz built-in)\n                \u2502\n7. sox records at native rate \u2192 raw PCM to stdout (no sox resampling)\n                \u2502\n8. Each 32ms chunk resampled to 16kHz, fed to Silero VAD + accumulated\n                \u2502\n9. Stop condition met:\n   - User: touch /tmp/voicelayer-stop\n   - Silero VAD: configurable silence after speech (0.5-2.5s)\n   - Pre-speech timeout: 15s of no speech detected\n   - Timeout: configurable (default 300s)\n                \u2502\n10. 16kHz PCM wrapped in WAV header \u2192 /tmp/voicelayer-recording-PID-TS.wav\n                \u2502\n11. whisper.cpp transcribes WAV \u2192 text\n                \u2502\n12. Text returned to agent, temp files cleaned up\n</code></pre>"},{"location":"architecture/overview/#external-dependencies","title":"External Dependencies","text":"<p>VoiceLayer delegates audio I/O to battle-tested system tools rather than bundling audio libraries:</p> Tool Purpose Install python3 + edge-tts Neural TTS (Microsoft, free) <code>pip3 install edge-tts</code> sox/rec Mic recording (native rate mono) <code>brew install sox</code> afplay (macOS) Audio playback Built-in mpv/ffplay/mpg123 (Linux) Audio playback Package manager whisper.cpp Local STT <code>brew install whisper-cpp</code> <p>This approach keeps VoiceLayer lightweight (~500 lines of TypeScript) while leveraging mature, well-tested audio tools.</p>"},{"location":"architecture/overview/#file-based-ipc","title":"File-Based IPC","text":"<p>VoiceLayer uses the filesystem for inter-process communication:</p> File Purpose Pattern <code>/tmp/voicelayer-session.lock</code> Mic mutex Atomic <code>wx</code> create, read JSON <code>/tmp/voicelayer-stop</code> Stop signal Touch to create, poll existence <code>/tmp/voicelayer-thinking.md</code> Think log Append-only markdown <p>This is intentional \u2014 MCP servers can't push UI updates to Claude Code. File-based signaling is the only reliable cross-process communication pattern available.</p>"},{"location":"architecture/session-booking/","title":"Session Booking","text":"<p>VoiceLayer uses lockfile-based session booking to prevent microphone conflicts when multiple Claude Code sessions run simultaneously.</p>"},{"location":"architecture/session-booking/#the-problem","title":"The Problem","text":"<p>You might have several Claude Code terminals open \u2014 one working on frontend, another on backend. If both try to record from the mic at the same time, audio gets corrupted and transcription fails.</p>"},{"location":"architecture/session-booking/#how-it-works","title":"How It Works","text":""},{"location":"architecture/session-booking/#lockfile-mutex","title":"Lockfile Mutex","text":"<p>Session booking uses an exclusive lockfile at <code>/tmp/voicelayer-session.lock</code>:</p> <pre><code>{\n  \"pid\": 12345,\n  \"sessionId\": \"mcp-12345\",\n  \"startedAt\": \"2026-02-21T10:30:00.000Z\"\n}\n</code></pre>"},{"location":"architecture/session-booking/#booking-flow","title":"Booking Flow","text":"<ol> <li>First <code>converse</code> call \u2014 auto-books if not already booked</li> <li>Stale lock check \u2014 if the lock exists but the PID is dead, remove it</li> <li>Atomic creation \u2014 uses <code>wx</code> file flag (exclusive create) to prevent race conditions</li> <li>Session-level \u2014 once booked, the session holds the mic for its entire lifetime</li> <li>Auto-release \u2014 lock released on process exit (SIGTERM, SIGINT, exit handlers)</li> </ol>"},{"location":"architecture/session-booking/#what-other-sessions-see","title":"What Other Sessions See","text":"<p>When a session tries to <code>converse</code> but the mic is booked by another:</p> <pre><code>[converse] Line is busy \u2014 voice session owned by mcp-12345\n(PID 12345) since 2026-02-21T10:30:00.000Z.\nFall back to text input, or wait for the other session to finish.\n</code></pre> <p>This returns with <code>isError: true</code>, allowing the agent to gracefully fall back to text-based interaction.</p>"},{"location":"architecture/session-booking/#non-blocking-modes-are-unaffected","title":"Non-Blocking Modes Are Unaffected","text":"<p>Announce, brief, consult, and think don't require mic access \u2014 they work regardless of session booking state. Only <code>converse</code> (which records audio) needs exclusive access.</p>"},{"location":"architecture/session-booking/#stale-lock-cleanup","title":"Stale Lock Cleanup","text":"<p>Locks can become stale if a process crashes without running cleanup handlers:</p> <ol> <li>On every booking check, VoiceLayer reads the lock and checks if the PID is alive</li> <li>Uses <code>process.kill(pid, 0)</code> \u2014 signal 0 checks process existence without sending a signal</li> <li><code>ESRCH</code> error = process is dead = stale lock = remove it</li> <li><code>EPERM</code> error = process exists but different user = treat as alive</li> </ol> <p>This means crashed sessions don't permanently block the mic.</p>"},{"location":"architecture/session-booking/#race-condition-safety","title":"Race Condition Safety","text":"<p>Two sessions might try to book simultaneously. VoiceLayer handles this with atomic file creation:</p> <pre><code>// Uses 'wx' flag \u2014 fails if file already exists (atomic)\nwriteFileSync(LOCK_FILE, JSON.stringify(lock), { flag: \"wx\" });\n</code></pre> <p>If both sessions race to create the lockfile, only one succeeds. The loser gets <code>EEXIST</code> and sees a \"line busy\" error.</p>"},{"location":"architecture/session-booking/#manual-lock-management","title":"Manual Lock Management","text":"<p>If something goes wrong, you can manually manage the lock:</p> <pre><code># Check who has the lock\ncat /tmp/voicelayer-session.lock\n\n# Force-release (only if the owning process is actually dead)\nrm /tmp/voicelayer-session.lock\n</code></pre> <p>Warning</p> <p>Only remove the lockfile if you're certain the owning process is dead. Removing an active lock can cause audio corruption.</p>"},{"location":"architecture/session-booking/#process-lifecycle","title":"Process Lifecycle","text":"<pre><code>MCP Server starts\n    \u2502\n    \u251c\u2500\u2500 Registers SIGTERM handler \u2192 releaseVoiceSession()\n    \u251c\u2500\u2500 Registers SIGINT handler  \u2192 releaseVoiceSession()\n    \u2514\u2500\u2500 Registers exit handler    \u2192 releaseVoiceSession()\n    \u2502\n    ... serves tool calls ...\n    \u2502\n    \u251c\u2500\u2500 First converse call \u2192 bookVoiceSession()\n    \u2502                          \u251c\u2500\u2500 cleanStaleLock()\n    \u2502                          \u251c\u2500\u2500 Check existing lock\n    \u2502                          \u2514\u2500\u2500 Atomic create lockfile\n    \u2502\n    ... more tool calls ...\n    \u2502\nProcess exits \u2192 cleanup runs\n    \u251c\u2500\u2500 releaseVoiceSession() (only if we own the lock)\n    \u2514\u2500\u2500 clearStopSignal() (remove /tmp/voicelayer-stop)\n</code></pre>"},{"location":"architecture/stt-backends/","title":"STT Backends","text":"<p>VoiceLayer supports two speech-to-text backends with automatic detection. Local processing via whisper.cpp is preferred; Wispr Flow provides a cloud fallback.</p>"},{"location":"architecture/stt-backends/#backend-comparison","title":"Backend Comparison","text":"Feature whisper.cpp Wispr Flow Type Local Cloud Speed ~200-400ms (Apple Silicon) ~500ms + network latency Privacy Audio never leaves your machine Audio sent to Wispr API Cost Free Requires API key Setup <code>brew install whisper-cpp</code> + model Set <code>QA_VOICE_WISPR_KEY</code> Quality Excellent (large-v3-turbo) Good Offline Yes No"},{"location":"architecture/stt-backends/#whispercpp-recommended","title":"whisper.cpp (Recommended)","text":"<p>Local-first, free, open-source. All processing on your machine \u2014 no cloud APIs, no data leaves your device.</p> <p>Binary name change in v1.8.3+</p> <p>Homebrew v1.8.3+ installs the binary as <code>whisper-cli</code> (was <code>whisper-cpp</code>). VoiceLayer auto-detects both names.</p>"},{"location":"architecture/stt-backends/#installation","title":"Installation","text":"<pre><code># macOS (Homebrew)\nbrew install whisper-cpp\n\n# Linux \u2014 build from source\ngit clone https://github.com/ggerganov/whisper.cpp\ncd whisper.cpp\ncmake -B build &amp;&amp; cmake --build build -j --config Release\nsudo cp build/bin/whisper-cli /usr/local/bin/whisper-cli\n</code></pre>"},{"location":"architecture/stt-backends/#model-download","title":"Model Download","text":"<pre><code>mkdir -p ~/.cache/whisper\n\n# Large v3 Turbo \u2014 best balance of speed and accuracy\ncurl -L -o ~/.cache/whisper/ggml-large-v3-turbo.bin \\\n  https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3-turbo.bin\n</code></pre> <p>Model comparison:</p> Model Size Speed (M1 Pro) Accuracy <code>ggml-large-v3-turbo</code> 1.6 GB ~200-400ms Best <code>ggml-large-v3-turbo-q5_0</code> ~1 GB ~150-300ms Very good <code>ggml-base.en</code> 148 MB ~50-100ms Good (English only) <code>ggml-small.en</code> 488 MB ~100-200ms Better (English only)"},{"location":"architecture/stt-backends/#auto-detection","title":"Auto-Detection","text":"<p>VoiceLayer scans <code>~/.cache/whisper/</code> for GGML model files in priority order:</p> <ol> <li><code>ggml-large-v3-turbo.bin</code></li> <li><code>ggml-large-v3-turbo-q5_0.bin</code></li> <li><code>ggml-base.en.bin</code></li> <li><code>ggml-base.bin</code></li> <li><code>ggml-small.en.bin</code></li> <li><code>ggml-small.bin</code></li> <li>Any other <code>ggml-*.bin</code> file</li> </ol> <p>Override with <code>QA_VOICE_WHISPER_MODEL=/path/to/model.bin</code>.</p>"},{"location":"architecture/stt-backends/#metal-acceleration","title":"Metal Acceleration","text":"<p>On macOS with Apple Silicon, whisper.cpp automatically uses Metal (GPU) acceleration. VoiceLayer detects the Homebrew prefix and sets <code>GGML_METAL_PATH_RESOURCES</code> so Metal shaders are found correctly.</p>"},{"location":"architecture/stt-backends/#transcription-flags","title":"Transcription Flags","text":"<p>VoiceLayer runs whisper.cpp with:</p> <ul> <li><code>--no-timestamps</code> \u2014 clean text output without <code>[00:00.000 --&gt; 00:05.000]</code> markers</li> <li><code>-l en</code> \u2014 English language</li> <li><code>--no-prints</code> \u2014 suppress progress output (stderr only)</li> </ul>"},{"location":"architecture/stt-backends/#wispr-flow-cloud-fallback","title":"Wispr Flow (Cloud Fallback)","text":""},{"location":"architecture/stt-backends/#setup","title":"Setup","text":"<ol> <li>Get an API key from Wispr Flow</li> <li>Set the environment variable:</li> </ol> <pre><code>export QA_VOICE_WISPR_KEY=\"your-api-key\"\n</code></pre> <p>Or in your MCP config:</p> <pre><code>{\n  \"mcpServers\": {\n    \"voicelayer\": {\n      \"command\": \"bunx\",\n      \"args\": [\"voicelayer-mcp\"],\n      \"env\": {\n        \"QA_VOICE_WISPR_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"architecture/stt-backends/#how-it-works","title":"How It Works","text":"<ol> <li>Recorded WAV audio is read into memory</li> <li>WAV header is stripped (44 bytes) to get raw PCM</li> <li>PCM is sent in 1-second chunks over WebSocket to Wispr API</li> <li>Each chunk includes base64 audio + RMS volume level</li> <li>A <code>commit</code> message signals end of audio</li> <li>Wispr returns transcribed text</li> <li>30-second timeout prevents hangs</li> </ol> <p>Privacy consideration</p> <p>Wispr Flow sends audio data to their cloud API. For sensitive conversations, use whisper.cpp (local) instead.</p>"},{"location":"architecture/stt-backends/#backend-selection","title":"Backend Selection","text":""},{"location":"architecture/stt-backends/#automatic-default","title":"Automatic (Default)","text":"<pre><code># No config needed \u2014 whisper.cpp if available, else Wispr Flow\nQA_VOICE_STT_BACKEND=auto  # this is the default\n</code></pre>"},{"location":"architecture/stt-backends/#force-whispercpp","title":"Force whisper.cpp","text":"<pre><code>QA_VOICE_STT_BACKEND=whisper\n</code></pre> <p>Fails with a clear error if whisper.cpp binary or model isn't found.</p>"},{"location":"architecture/stt-backends/#force-wispr-flow","title":"Force Wispr Flow","text":"<pre><code>QA_VOICE_STT_BACKEND=wispr\n</code></pre> <p>Fails with a clear error if <code>QA_VOICE_WISPR_KEY</code> isn't set.</p>"},{"location":"architecture/stt-backends/#stt-result-format","title":"STT Result Format","text":"<p>Both backends return the same structure:</p> <pre><code>interface STTResult {\n  text: string;       // Transcribed text\n  backend: string;    // \"whisper.cpp\" or \"wispr-flow\"\n  durationMs: number; // Transcription time in milliseconds\n}\n</code></pre> <p>The backend name and duration are logged to stderr for debugging.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>VoiceLayer is configured entirely via environment variables. All settings have sensible defaults \u2014 zero config required for basic usage.</p>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/configuration/#stt-speech-to-text","title":"STT (Speech-to-Text)","text":"Variable Default Description <code>QA_VOICE_STT_BACKEND</code> <code>auto</code> Backend selection: <code>whisper</code>, <code>wispr</code>, or <code>auto</code> <code>QA_VOICE_WHISPER_MODEL</code> auto-detected Absolute path to a whisper.cpp GGML model file <code>QA_VOICE_WISPR_KEY</code> \u2014 Wispr Flow API key (cloud fallback only) <p>Auto-detection (<code>auto</code> mode) checks for whisper.cpp first, falls back to Wispr Flow if <code>QA_VOICE_WISPR_KEY</code> is set.</p> <p>Model auto-detection scans <code>~/.cache/whisper/</code> for GGML files in this order:</p> <ol> <li><code>ggml-large-v3-turbo.bin</code></li> <li><code>ggml-large-v3-turbo-q5_0.bin</code></li> <li><code>ggml-base.en.bin</code></li> <li><code>ggml-base.bin</code></li> <li><code>ggml-small.en.bin</code></li> <li><code>ggml-small.bin</code></li> <li>Any other <code>ggml-*.bin</code> file</li> </ol>"},{"location":"getting-started/configuration/#tts-text-to-speech","title":"TTS (Text-to-Speech)","text":"Variable Default Description <code>QA_VOICE_TTS_VOICE</code> <code>en-US-JennyNeural</code> Microsoft edge-tts voice ID <code>QA_VOICE_TTS_RATE</code> <code>+0%</code> Base speech rate (per-mode defaults layer on top) <p>Available voices \u2014 run <code>edge-tts --list-voices</code> for the full list. Popular choices:</p> Voice Language Style <code>en-US-JennyNeural</code> English (US) Default, clear female <code>en-US-GuyNeural</code> English (US) Male <code>en-GB-SoniaNeural</code> English (UK) British female <code>en-US-AriaNeural</code> English (US) Expressive female"},{"location":"getting-started/configuration/#recording","title":"Recording","text":"<p>Recording uses Silero VAD (neural network) for speech detection. The device's native sample rate is auto-detected \u2014 no configuration needed for any microphone (built-in, AirPods, USB, etc.).</p> <p>Silence detection is configured per-call via the <code>silence_mode</code> parameter on <code>voice_ask</code>:</p> Mode Silence Duration Use Case <code>quick</code> 0.5s Fast responses, short answers <code>standard</code> 1.5s Normal conversation <code>thoughtful</code> 2.5s (default) User pauses to think"},{"location":"getting-started/configuration/#output","title":"Output","text":"Variable Default Description <code>QA_VOICE_THINK_FILE</code> <code>/tmp/voicelayer-thinking.md</code> Path for the think mode markdown log"},{"location":"getting-started/configuration/#per-mode-speech-rates","title":"Per-Mode Speech Rates","text":"<p>Each voice mode has a default rate that balances speed with clarity:</p> Mode Default Rate Rationale announce <code>+10%</code> Quick status updates \u2014 snappy delivery brief <code>-10%</code> Long explanations \u2014 slower for digestion consult <code>+5%</code> Checkpoints \u2014 slightly fast, user may respond converse <code>+0%</code> Conversational \u2014 natural speed <p>Rates are auto-adjusted for long text:</p> Text Length Adjustment &lt; 300 chars No change 300-599 chars -5% 600-999 chars -10% 1000+ chars -15% <p>You can override per-call by passing the <code>rate</code> parameter to any TTS tool.</p>"},{"location":"getting-started/configuration/#mcp-server-configuration","title":"MCP Server Configuration","text":""},{"location":"getting-started/configuration/#basic-setup","title":"Basic Setup","text":"<pre><code>{\n  \"mcpServers\": {\n    \"voicelayer\": {\n      \"command\": \"bunx\",\n      \"args\": [\"voicelayer-mcp\"]\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#with-environment-overrides","title":"With Environment Overrides","text":"<pre><code>{\n  \"mcpServers\": {\n    \"voicelayer\": {\n      \"command\": \"bunx\",\n      \"args\": [\"voicelayer-mcp\"],\n      \"env\": {\n        \"QA_VOICE_TTS_VOICE\": \"en-GB-SoniaNeural\",\n        \"QA_VOICE_STT_BACKEND\": \"whisper\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#file-paths","title":"File Paths","text":"<p>VoiceLayer uses <code>/tmp</code> for all runtime files:</p> File Purpose <code>/tmp/voicelayer-session.lock</code> Session booking lockfile <code>/tmp/voicelayer-stop</code> User stop signal (touch to end) <code>/tmp/voicelayer-tts-*.mp3</code> Temporary TTS audio (auto-cleaned) <code>/tmp/voicelayer-recording-*.wav</code> Temporary recording (auto-cleaned) <code>/tmp/voicelayer-thinking.md</code> Think mode log (persistent until cleared)"},{"location":"getting-started/prerequisites/","title":"Prerequisites","text":"<p>Everything VoiceLayer needs, with one-liner installs for each platform.</p>"},{"location":"getting-started/prerequisites/#required","title":"Required","text":""},{"location":"getting-started/prerequisites/#bun-javascript-runtime","title":"Bun (JavaScript runtime)","text":"<p>VoiceLayer runs on Bun. Install it with:</p> <pre><code>curl -fsSL https://bun.sh/install | bash\n</code></pre> <p>Verify: <code>bun --version</code> should print <code>1.x.x</code> or higher.</p>"},{"location":"getting-started/prerequisites/#sox-microphone-recording","title":"sox (microphone recording)","text":"<p>sox provides the <code>rec</code> command used to capture audio from your microphone.</p> macOSUbuntu/DebianFedora/RHEL <pre><code>brew install sox\n</code></pre> <pre><code>sudo apt install sox\n</code></pre> <pre><code>sudo dnf install sox\n</code></pre> <p>Verify: <code>rec --version</code> should print version info.</p>"},{"location":"getting-started/prerequisites/#edge-tts-text-to-speech","title":"edge-tts (text-to-speech)","text":"<p>Microsoft's neural TTS engine. Free, no API key needed.</p> <pre><code>pip3 install edge-tts\n</code></pre> <p>Verify: <code>python3 -m edge_tts --list-voices</code> should print a list of voices.</p> <p>Python 3 required</p> <p>edge-tts is a Python package. Most systems have Python 3 pre-installed. If not: <code>brew install python3</code> (macOS) or <code>sudo apt install python3-pip</code> (Linux).</p>"},{"location":"getting-started/prerequisites/#claude-code","title":"Claude Code","text":"<p>VoiceLayer is an MCP server for Claude Code. Install Claude Code from Anthropic's docs.</p>"},{"location":"getting-started/prerequisites/#recommended","title":"Recommended","text":""},{"location":"getting-started/prerequisites/#whispercpp-local-speech-to-text","title":"whisper.cpp (local speech-to-text)","text":"<p>Local transcription \u2014 fast on Apple Silicon (~300ms for a 5-second clip), no cloud dependency.</p> macOSLinux <pre><code>brew install whisper-cpp\n</code></pre> <p>Build from source \u2014 see the whisper.cpp repo.</p> <p>Then download a model:</p> <pre><code>mkdir -p ~/.cache/whisper\ncurl -L -o ~/.cache/whisper/ggml-large-v3-turbo.bin \\\n  https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3-turbo.bin\n</code></pre> <p>Smaller models available</p> <p>The large-v3-turbo model (~1.5 GB) gives the best accuracy. For faster downloads, use <code>ggml-base.en.bin</code> (~142 MB) \u2014 English only, slightly less accurate.</p> <p>VoiceLayer auto-detects models in <code>~/.cache/whisper/</code>. No config needed.</p>"},{"location":"getting-started/prerequisites/#audio-player-linux-only","title":"Audio player (Linux only)","text":"<p>macOS uses the built-in <code>afplay</code>. Linux needs one of these for MP3 playback:</p> <pre><code>sudo apt install mpv    # recommended\n# or: sudo apt install mpg123\n# or: sudo apt install ffmpeg  (provides ffplay)\n</code></pre>"},{"location":"getting-started/prerequisites/#optional","title":"Optional","text":""},{"location":"getting-started/prerequisites/#wispr-flow-cloud-stt-fallback","title":"Wispr Flow (cloud STT fallback)","text":"<p>If you don't install whisper.cpp, VoiceLayer can use Wispr Flow as a cloud-based speech-to-text backend. Requires an API key:</p> <pre><code>export QA_VOICE_WISPR_KEY=\"your-api-key\"\n</code></pre> <p>This is optional \u2014 whisper.cpp is preferred for speed and privacy.</p>"},{"location":"getting-started/prerequisites/#microphone-access-macos","title":"Microphone Access (macOS)","text":"<p>On macOS, your terminal app needs microphone permission:</p> <p>System Settings &gt; Privacy &amp; Security &gt; Microphone \u2014 enable your terminal (iTerm2, Terminal.app, Warp, etc.)</p> <p>First recording may prompt</p> <p>The first time VoiceLayer tries to record, macOS will show a permission dialog. Grant it, then try again.</p>"},{"location":"getting-started/prerequisites/#quick-check","title":"Quick Check","text":"<p>Run these to verify everything is ready:</p> <pre><code>bun --version          # Should print 1.x.x+\nrec --version          # Should print sox version info\npython3 -m edge_tts -h # Should print help text\nwhisper-cli --help     # Should print help (optional, v1.8.3+ binary name)\n</code></pre> <p>If all commands work, head to the Quick Start to connect VoiceLayer to Claude Code.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get VoiceLayer running with Claude Code in 5 minutes.</p>"},{"location":"getting-started/quickstart/#1-install-prerequisites","title":"1. Install Prerequisites","text":"macOS (one block)Linux (one block) <pre><code>brew install sox whisper-cpp\npip3 install edge-tts\ncurl -fsSL https://bun.sh/install | bash\n</code></pre> <pre><code>sudo apt install sox mpv\npip3 install edge-tts\ncurl -fsSL https://bun.sh/install | bash\n</code></pre> <p>Need details? See the full Prerequisites page.</p>"},{"location":"getting-started/quickstart/#2-download-a-whisper-model","title":"2. Download a Whisper Model","text":"<p>whisper.cpp needs a model file for local speech-to-text:</p> <pre><code>mkdir -p ~/.cache/whisper\ncurl -L -o ~/.cache/whisper/ggml-large-v3-turbo.bin \\\n  https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3-turbo.bin\n</code></pre> <p>This is a ~1.5 GB download</p> <p>For a smaller model (~142 MB, English only): replace <code>ggml-large-v3-turbo.bin</code> with <code>ggml-base.en.bin</code>.</p>"},{"location":"getting-started/quickstart/#3-add-to-claude-code","title":"3. Add to Claude Code","text":"<p>Add VoiceLayer to your MCP config. Open (or create) <code>.mcp.json</code> in your project root or <code>~/.claude/.mcp.json</code> for global access:</p> <pre><code>{\n  \"mcpServers\": {\n    \"voicelayer\": {\n      \"command\": \"bunx\",\n      \"args\": [\"voicelayer-mcp\"]\n    }\n  }\n}\n</code></pre> <p>That's it. The next time you start Claude Code, VoiceLayer will be available.</p> Alternative: install from source <pre><code>git clone https://github.com/EtanHey/voicelayer.git\ncd voicelayer &amp;&amp; bun install\n</code></pre> <p>Then use this MCP config instead:</p> <pre><code>{\n  \"mcpServers\": {\n    \"voicelayer\": {\n      \"command\": \"bun\",\n      \"args\": [\"run\", \"/absolute/path/to/voicelayer/src/mcp-server.ts\"]\n    }\n  }\n}\n</code></pre> <p>The <code>args</code> path must be absolute \u2014 MCP servers don't inherit your shell's working directory.</p>"},{"location":"getting-started/quickstart/#4-grant-microphone-access-macos","title":"4. Grant Microphone Access (macOS)","text":"<p>On macOS, your terminal app needs mic permission:</p> <p>System Settings &gt; Privacy &amp; Security &gt; Microphone \u2014 enable your terminal (iTerm2, Terminal.app, Warp, etc.)</p>"},{"location":"getting-started/quickstart/#5-test-it","title":"5. Test It","text":"<p>Start a Claude Code session and try:</p> <pre><code>Claude, announce \"VoiceLayer is working\"\n</code></pre> <p>You should hear the message spoken aloud. For full voice conversation:</p> <pre><code>Claude, use converse mode to ask me how the UI looks\n</code></pre> <p>Claude will speak the question, record your voice response, and continue the conversation.</p>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>What is VoiceLayer? \u2014 non-technical overview with examples</li> <li>Voice Modes \u2014 when to use announce vs. brief vs. converse</li> <li>Configuration \u2014 change the voice, speech rate, STT backend</li> <li>MCP Tools Reference \u2014 full tool API with parameters</li> </ul>"},{"location":"modes/announce/","title":"Announce Mode","text":"<p>Fire-and-forget text-to-speech. The agent speaks a short message aloud \u2014 no response expected.</p>"},{"location":"modes/announce/#when-to-use","title":"When to Use","text":"<ul> <li>Task completion alerts: \"Deploy finished successfully\"</li> <li>Status updates: \"Running tests now, 12 of 47 complete\"</li> <li>Narration during long operations: \"Moving on to the database migration\"</li> </ul>"},{"location":"modes/announce/#mcp-tool","title":"MCP Tool","text":"<p>Tool: <code>voice_speak</code> with <code>mode: \"announce\"</code> (or omit mode \u2014 short messages default to announce) Alias: <code>qa_voice_announce</code>, <code>qa_voice_say</code></p>"},{"location":"modes/announce/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>message</code> string Yes \u2014 Text to speak aloud (non-empty after trimming) <code>rate</code> string No <code>+10%</code> Speech rate override (e.g., <code>-5%</code>, <code>+15%</code>)"},{"location":"modes/announce/#returns","title":"Returns","text":"<pre><code>{\n  \"content\": [{ \"type\": \"text\", \"text\": \"[announce] Spoke: \\\"your message\\\"\" }]\n}\n</code></pre>"},{"location":"modes/announce/#errors","title":"Errors","text":"Error Cause Fix Missing message Empty or missing <code>message</code> param Provide non-empty message edge-tts failed edge-tts not installed <code>pip3 install edge-tts</code> Audio playback failed No audio player found macOS: built-in. Linux: install <code>mpv</code>"},{"location":"modes/announce/#behavior","title":"Behavior","text":"<ol> <li>Text is synthesized via edge-tts to a temporary MP3</li> <li>MP3 plays through system speakers</li> <li>Temp file is cleaned up</li> <li>Returns immediately after playback completes</li> </ol> <p>No mic recording, no session booking, no blocking.</p>"},{"location":"modes/announce/#speech-rate","title":"Speech Rate","text":"<p>Default: +10% (snappy delivery for short updates).</p> <p>Auto-adjusted for long text:</p> <ul> <li>300-599 chars: +5%</li> <li>600-999 chars: +0%</li> <li>1000+ chars: -5%</li> </ul> <p>Override with the <code>rate</code> parameter:</p> <pre><code>voice_speak({ message: \"Important update\", mode: \"announce\", rate: \"-10%\" })\n</code></pre>"},{"location":"modes/announce/#stop-signal","title":"Stop Signal","text":"<p>User can end playback early:</p> <pre><code>touch /tmp/voicelayer-stop\n</code></pre> <p>The stop file is cleaned up automatically after detection.</p>"},{"location":"modes/brief/","title":"Brief Mode","text":"<p>One-way explanation via TTS. The agent reads back a longer piece of content \u2014 decisions, summaries, findings \u2014 while the user listens. No response expected.</p>"},{"location":"modes/brief/#when-to-use","title":"When to Use","text":"<ul> <li>Reading back analysis results: \"Here's what I found in the codebase...\"</li> <li>Summarizing a plan: \"The migration will happen in three phases...\"</li> <li>Explaining a decision: \"I chose Redis over Memcached because...\"</li> </ul>"},{"location":"modes/brief/#mcp-tool","title":"MCP Tool","text":"<p>Tool: <code>voice_speak</code> with <code>mode: \"brief\"</code> (or auto-selected for messages &gt;280 chars) Alias: <code>qa_voice_brief</code></p>"},{"location":"modes/brief/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>message</code> string Yes \u2014 The explanation or summary to speak (non-empty after trimming) <code>rate</code> string No <code>-10%</code> Speech rate override (e.g., <code>-20%</code>, <code>+0%</code>)"},{"location":"modes/brief/#returns","title":"Returns","text":"<pre><code>{\n  \"content\": [{ \"type\": \"text\", \"text\": \"[brief] Explained: \\\"your message\\\"\" }]\n}\n</code></pre>"},{"location":"modes/brief/#errors","title":"Errors","text":"<p>Same as Announce \u2014 edge-tts or audio player issues.</p>"},{"location":"modes/brief/#behavior","title":"Behavior","text":"<p>Identical pipeline to announce (edge-tts -&gt; audio player -&gt; cleanup), but with a slower default rate optimized for longer content.</p>"},{"location":"modes/brief/#speech-rate","title":"Speech Rate","text":"<p>Default: -10% (slower for comprehension of longer content).</p> <p>Auto-adjusted for text length:</p> <ul> <li>300-599 chars: -15%</li> <li>600-999 chars: -20%</li> <li>1000+ chars: -25%</li> </ul> <p>This makes brief mode noticeably slower than announce \u2014 intentional, since brief content is typically 2-5 sentences that the user needs to absorb.</p>"},{"location":"modes/brief/#stop-signal","title":"Stop Signal","text":"<p>Same as all TTS modes:</p> <pre><code>touch /tmp/voicelayer-stop\n</code></pre>"},{"location":"modes/consult/","title":"Consult Mode","text":"<p>Speak a checkpoint message \u2014 the user may want to respond, but no mic recording happens. Use for preemptive checkpoints before important actions.</p>"},{"location":"modes/consult/#when-to-use","title":"When to Use","text":"<ul> <li>Before destructive actions: \"About to drop the staging database. Okay?\"</li> <li>Before commits/pushes: \"I'm ready to push to main. Want to review first?\"</li> <li>Decision points: \"Should I use Redis or the built-in cache?\"</li> </ul>"},{"location":"modes/consult/#mcp-tool","title":"MCP Tool","text":"<p>Tool: <code>voice_speak</code> with <code>mode: \"consult\"</code> (or auto-selected for messages with \"?\" or \"about to\") Alias: <code>qa_voice_consult</code></p>"},{"location":"modes/consult/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>message</code> string Yes \u2014 The checkpoint question or status (non-empty after trimming) <code>rate</code> string No <code>+5%</code> Speech rate override"},{"location":"modes/consult/#returns","title":"Returns","text":"<pre><code>{\n  \"content\": [{\n    \"type\": \"text\",\n    \"text\": \"[consult] Spoke: \\\"your message\\\"\\nUser may want to respond. Use voice_ask to collect voice input if needed.\"\n  }]\n}\n</code></pre>"},{"location":"modes/consult/#errors","title":"Errors","text":"<p>Same as Announce \u2014 edge-tts or audio player issues.</p>"},{"location":"modes/consult/#behavior","title":"Behavior","text":"<ol> <li>Text is spoken aloud via edge-tts (same as announce/brief)</li> <li>Returns immediately \u2014 no mic recording</li> <li>The return text hints that the agent should follow up with <code>voice_ask</code> if input is needed</li> </ol> <p>Consult is a \"heads up\" \u2014 it doesn't block. If the user says nothing (types nothing), the agent can proceed. If the user wants to respond verbally, the agent should call <code>voice_ask</code> as a follow-up.</p>"},{"location":"modes/consult/#speech-rate","title":"Speech Rate","text":"<p>Default: +5% (slightly faster than natural \u2014 checkpoints should be brisk).</p>"},{"location":"modes/consult/#when-to-follow-up-with-voice_ask","title":"When to Follow Up with voice_ask","text":"<p>The consult tool's response includes a hint:</p> <p>\"User may want to respond. Use voice_ask to collect voice input if needed.\"</p> <p>The agent should decide based on context whether to:</p> <ol> <li>Wait for text input \u2014 if the user typically types</li> <li>Call voice_ask \u2014 if in a voice session and input is expected</li> <li>Proceed \u2014 if the checkpoint was informational only</li> </ol>"},{"location":"modes/converse/","title":"Converse Mode","text":"<p>Full bidirectional voice Q&amp;A. The agent speaks a question, records the user's voice response via microphone, transcribes it with whisper.cpp (or Wispr Flow), and returns the text.</p> <p>This is the only blocking mode \u2014 the tool call doesn't return until the user finishes speaking or the timeout expires.</p>"},{"location":"modes/converse/#when-to-use","title":"When to Use","text":"<ul> <li>Interactive Q&amp;A sessions: \"What did you think of the prototype?\"</li> <li>Drilling sessions: \"Walk me through how the auth flow works\"</li> <li>Discovery calls: \"What are the main pain points with the current system?\"</li> <li>QA testing: \"How does the checkout page look on your screen?\"</li> </ul>"},{"location":"modes/converse/#mcp-tool","title":"MCP Tool","text":"<p>Tool: <code>voice_ask</code> (blocking voice Q&amp;A) Alias: <code>qa_voice_converse</code>, <code>qa_voice_ask</code></p>"},{"location":"modes/converse/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>message</code> string Yes \u2014 Question or prompt to speak aloud (non-empty) <code>timeout_seconds</code> number No <code>300</code> Max wait time for response (clamped to 10-3600)"},{"location":"modes/converse/#returns","title":"Returns","text":"<p>On success \u2014 the user's transcribed text:</p> <pre><code>{\n  \"content\": [{ \"type\": \"text\", \"text\": \"The hamburger menu is cut off on mobile\" }]\n}\n</code></pre> <p>On timeout / no speech:</p> <pre><code>{\n  \"content\": [{ \"type\": \"text\", \"text\": \"[converse] No response received within 300 seconds. The user may have stepped away.\" }]\n}\n</code></pre> <p>On busy (another session has the mic):</p> <pre><code>{\n  \"content\": [{ \"type\": \"text\", \"text\": \"[converse] Line is busy \u2014 voice session owned by mcp-12345 (PID 12345) since 2026-02-21T10:00:00Z. Fall back to text input, or wait for the other session to finish.\" }],\n  \"isError\": true\n}\n</code></pre>"},{"location":"modes/converse/#errors","title":"Errors","text":"Error Cause Fix Line busy Another session has the mic Wait or fall back to text sox not installed <code>rec</code> command not found <code>brew install sox</code> Mic permission denied Terminal not authorized macOS: System Settings &gt; Privacy &gt; Microphone No STT backend Neither whisper.cpp nor Wispr available <code>brew install whisper-cpp</code> (binary: <code>whisper-cli</code>) or set <code>QA_VOICE_WISPR_KEY</code>"},{"location":"modes/converse/#behavior","title":"Behavior","text":""},{"location":"modes/converse/#full-flow","title":"Full Flow","text":"<ol> <li>Session booking \u2014 auto-books if not already booked (lockfile check)</li> <li>Clear state \u2014 removes leftover input/stop signals</li> <li>Wait for prior audio \u2014 auto-waits for any playing <code>voice_speak</code> audio to finish (prevents overlap)</li> <li>Speak question \u2014 edge-tts synthesizes and plays the question (blocking \u2014 waits for playback)</li> <li>Detect device rate \u2014 probes default audio input for native sample rate (e.g., 24kHz for AirPods, 48kHz for built-in mic)</li> <li>Record mic \u2014 sox records at device's native rate, audio resampled to 16kHz in real-time</li> <li>Silero VAD \u2014 neural network detects speech vs. noise in each 32ms chunk</li> <li>Wait for stop \u2014 user stop signal (primary), VAD silence detection (configurable), or timeout</li> <li>Transcribe \u2014 whisper.cpp or Wispr Flow converts 16kHz audio to text</li> <li>Return \u2014 transcribed text returned to the agent</li> </ol>"},{"location":"modes/converse/#stop-methods-in-priority-order","title":"Stop Methods (in priority order)","text":"<ol> <li>User stop signal (PRIMARY): <code>touch /tmp/voicelayer-stop</code></li> <li>Silero VAD silence detection (FALLBACK): configurable silence duration after speech is detected</li> <li>Pre-speech timeout: 15s of no speech \u2192 returns null early</li> <li>Timeout (SAFETY NET): <code>timeout_seconds</code> parameter (default 300s)</li> </ol> <p>Why user-controlled stop is primary</p> <p>Silence detection can misfire \u2014 background noise, thinking pauses, or mic sensitivity issues cause premature cutoff. The touch-file approach gives the user explicit control over when they're done speaking.</p>"},{"location":"modes/converse/#session-booking","title":"Session Booking","text":"<p>Converse mode requires exclusive mic access. The first <code>voice_ask</code> call auto-books a session:</p> <ul> <li>Lockfile: <code>/tmp/voicelayer-session.lock</code></li> <li>Contains: PID, session ID, start timestamp</li> <li>Stale lock cleanup: dead PIDs are auto-detected and removed</li> <li>Race condition safe: uses atomic exclusive file creation (<code>wx</code> flag)</li> </ul> <p>Other Claude Code sessions that try to use <code>voice_ask</code> see \"line busy\" and should fall back to text input.</p> <p>The lock is released when the MCP server process exits (SIGTERM/SIGINT/exit handlers).</p>"},{"location":"modes/converse/#recording-details","title":"Recording Details","text":"Setting Value Recording rate Device native (auto-detected, e.g., 24kHz, 48kHz) Output rate 16,000 Hz (resampled in code) Channels 1 (mono) Bit depth 16-bit signed Format Raw PCM \u2192 resample \u2192 WAV VAD Silero VAD v5 (neural network, 32ms chunks) Default silence mode Thoughtful (2.5s of silence after speech) <p>Audio is recorded at the device's native sample rate via sox to avoid buffer overruns, resampled to 16kHz in real-time via linear interpolation, processed by Silero VAD for speech detection, then wrapped in a WAV header and passed to the STT backend.</p> <p>AirPods and Bluetooth devices</p> <p>Bluetooth audio devices (e.g., AirPods) often only support specific sample rates (24kHz). VoiceLayer auto-detects the device rate and handles resampling transparently \u2014 no configuration needed.</p>"},{"location":"modes/converse/#speech-rate","title":"Speech Rate","text":"<p>Default: +0% (natural conversational pace).</p> <p>No auto-slowdown applied \u2014 converse questions are typically short.</p>"},{"location":"modes/overview/","title":"Voice Modes Guide","text":"<p>VoiceLayer has 2 tools: voice_speak (output) and voice_ask (input). voice_speak supports 5 modes \u2014 auto-detected from message content or set explicitly.</p>"},{"location":"modes/overview/#quick-reference","title":"Quick Reference","text":"Mode Tool Claude Speaks You Speak Use For Announce voice_speak Yes No Status updates, notifications Brief voice_speak Yes No Explanations, reading back findings Consult voice_speak Yes No Checkpoints before actions Converse voice_ask Yes Yes Interactive Q&amp;A, discussions Think voice_speak No No Silent notes to a log file"},{"location":"modes/overview/#choosing-the-right-mode","title":"Choosing the Right Mode","text":""},{"location":"modes/overview/#claude-just-needs-to-tell-me-something","title":"\"Claude just needs to tell me something\"","text":"<p>Use announce for short updates:</p> <p>\"Build complete. All tests passing.\" \"Deployed to staging.\" \"Found 3 issues on the checkout page.\"</p> <p>Use brief for longer explanations:</p> <p>\"Here's what I found in the auth module. There are three entry points: the main middleware validates JWT tokens, the second handles OAuth callbacks, and the third manages API key auth for service accounts.\"</p> <p>The difference: announce is snappy (+10% speed), brief is slower (-10%) so you can follow along.</p>"},{"location":"modes/overview/#claude-should-check-with-me-first","title":"\"Claude should check with me first\"","text":"<p>Use consult before important actions:</p> <p>\"I'm about to push to main and create a PR. Want me to go ahead?\" \"The test suite has 3 failures. Should I fix them or skip for now?\"</p> <p>Consult is one-way \u2014 Claude speaks but doesn't record your mic. If you want to respond by voice, tell Claude to follow up with voice_ask.</p>"},{"location":"modes/overview/#we-need-to-have-a-conversation","title":"\"We need to have a conversation\"","text":"<p>Use converse for back-and-forth voice dialogue:</p> <p>Claude: \"How does the navigation look on mobile?\" You: \"The hamburger menu is cut off on the right side, and the dropdown overlaps with the hero section.\"</p> <p>This is the only mode that records your microphone. Claude speaks the question, waits for your voice response, transcribes it locally, and continues.</p> <p>Stopping a recording: Touch <code>/tmp/voicelayer-stop-{TOKEN}</code> or wait for 2.5s of silence (thoughtful default).</p>"},{"location":"modes/overview/#claude-should-take-notes-silently","title":"\"Claude should take notes silently\"","text":"<p>Use think for background note-taking with no audio:</p> <p>Insight: User mentioned they prefer dark mode defaults Red flag: No error handling in the payment flow Question: Should we support Safari &lt; 16?</p> <p>Think mode writes timestamped entries to <code>/tmp/voicelayer-thinking.md</code>. Useful for review sessions where Claude captures observations without interrupting the flow.</p>"},{"location":"modes/overview/#speech-rates","title":"Speech Rates","text":"<p>Each mode has a tuned speed:</p> Mode Default Rate Why Announce +10% Quick updates, no need to linger Brief -10% Longer content needs slower delivery Consult +5% Checkpoints \u2014 slightly fast Converse +0% Natural conversational pace <p>All modes auto-slow for text longer than 300 characters.</p> <p>You can override the rate per-call or globally via the <code>QA_VOICE_TTS_RATE</code> environment variable. See Configuration.</p>"},{"location":"modes/overview/#stop-signal","title":"Stop Signal","text":"<p>All audio modes support an immediate stop:</p> <pre><code>touch /tmp/voicelayer-stop\n</code></pre> <p>This kills playback instantly. In converse mode, it also ends mic recording and sends whatever was captured so far for transcription.</p>"},{"location":"modes/overview/#session-booking","title":"Session Booking","text":"<p>Only converse mode uses the microphone. To prevent conflicts when running multiple Claude Code sessions, VoiceLayer uses a lockfile (<code>/tmp/voicelayer-session.lock</code>).</p> <ul> <li>First session to call converse books the mic</li> <li>Other sessions see \"line busy\" and fall back to text</li> <li>Lock auto-cleans if the owning process dies</li> <li>Lock releases on session exit</li> </ul> <p>See Session Booking for details.</p>"},{"location":"modes/overview/#aliases","title":"Aliases","text":"<p>Old <code>qa_voice_*</code> names still work as backward-compat aliases (e.g. <code>qa_voice_say</code> \u2192 announce, <code>qa_voice_ask</code> \u2192 voice_ask).</p>"},{"location":"modes/think/","title":"Think Mode","text":"<p>Silent note-taking to a markdown log file. No audio, no mic \u2014 the agent captures insights, questions, and red flags that the user can view in a split-screen editor.</p>"},{"location":"modes/think/#when-to-use","title":"When to Use","text":"<ul> <li>During discovery calls: silently track unknowns and red flags</li> <li>During QA sessions: note patterns without interrupting the flow</li> <li>During code review: capture observations for later discussion</li> </ul>"},{"location":"modes/think/#mcp-tool","title":"MCP Tool","text":"<p>Tool: <code>voice_speak</code> with <code>mode: \"think\"</code> (or auto-selected for \"insight:\", \"note:\", \"TODO:\") Alias: <code>qa_voice_think</code> (uses <code>thought</code> param; voice_speak uses <code>message</code>)</p>"},{"location":"modes/think/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>message</code> / <code>thought</code> string Yes \u2014 The insight, suggestion, or note to append <code>category</code> string No <code>insight</code> One of: <code>insight</code>, <code>question</code>, <code>red-flag</code>, <code>checklist-update</code>"},{"location":"modes/think/#returns","title":"Returns","text":"<pre><code>{\n  \"content\": [{ \"type\": \"text\", \"text\": \"Noted (insight): The auth flow has a race condition\" }]\n}\n</code></pre>"},{"location":"modes/think/#errors","title":"Errors","text":"Error Cause Fix Missing thought Empty or missing <code>thought</code> param Provide non-empty text File write failure Permissions on think file path Check path in <code>QA_VOICE_THINK_FILE</code>"},{"location":"modes/think/#behavior","title":"Behavior","text":"<ol> <li>Formats the thought with timestamp, category icon, and text</li> <li>Appends to the think log file</li> <li>Returns confirmation text</li> </ol> <p>The file is created on first write with a <code># Live Thinking Log</code> header.</p>"},{"location":"modes/think/#categories","title":"Categories","text":"Category Icon When to Use <code>insight</code> :bulb: Observations, patterns, connections <code>question</code> :question: Things to ask about or investigate <code>red-flag</code> :triangular_flag_on_post: Concerns, risks, warnings <code>checklist-update</code> :white_check_mark: Progress on a checklist or task"},{"location":"modes/think/#output-format","title":"Output Format","text":"<p>The think log is a simple markdown file:</p> <pre><code># Live Thinking Log\n\n- [10:32] (bulb icon) The auth module uses JWT but doesn't validate expiry\n- [10:33] (question icon) Why is there a second database connection pool?\n- [10:35] (flag icon) No rate limiting on the login endpoint\n- [10:37] (checkmark icon) Navigation: mobile responsive - PASS\n</code></pre>"},{"location":"modes/think/#configuration","title":"Configuration","text":"Variable Default Description <code>QA_VOICE_THINK_FILE</code> <code>/tmp/voicelayer-thinking.md</code> Path to the thinking log <p>To persist the log across reboots, set <code>QA_VOICE_THINK_FILE</code> to a non-<code>/tmp</code> path:</p> <pre><code>{\n  \"mcpServers\": {\n    \"voicelayer\": {\n      \"command\": \"bunx\",\n      \"args\": [\"voicelayer-mcp\"],\n      \"env\": {\n        \"QA_VOICE_THINK_FILE\": \"/Users/me/notes/voicelayer-thinking.md\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"modes/think/#viewing-the-log","title":"Viewing the Log","text":"<p>Open the think file in a split-screen editor while your Claude Code session runs:</p> <pre><code># Watch for updates in real-time\ntail -f /tmp/voicelayer-thinking.md\n\n# Or open in your editor's split pane\ncode /tmp/voicelayer-thinking.md\n</code></pre>"},{"location":"plans/2026-02-24-flow-bar-design/","title":"Flow Bar \u2014 VoiceLayer Floating Widget","text":"<p>Native macOS SwiftUI app that shows voice state and provides stop/toggle/replay controls. Communicates with VoiceLayer MCP server via Unix domain socket.</p>"},{"location":"plans/2026-02-24-flow-bar-design/#vision","title":"Vision","text":"<p>Free, open-source Wispr Flow alternative focused on Claude Code. A floating pill at the bottom of the screen that shows what VoiceLayer is doing (speaking, recording, transcribing) and lets the user control it without touching terminal commands.</p>"},{"location":"plans/2026-02-24-flow-bar-design/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       Unix socket        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  VoiceLayer MCP     \u2502  \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502  Flow Bar        \u2502\n\u2502  (Bun/TypeScript)   \u2502  /tmp/voicelayer.sock     \u2502  (SwiftUI app)   \u2502\n\u2502                     \u2502                           \u2502                  \u2502\n\u2502  Creates socket     \u2502  JSON newline-delimited   \u2502  Reconnecting    \u2502\n\u2502  Sends state events \u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba    \u2502  client          \u2502\n\u2502  Receives commands  \u2502  \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2502  Sends commands  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Socket ownership: VoiceLayer creates <code>/tmp/voicelayer.sock</code> on startup. Flow Bar connects as a client and reconnects on disconnect. If VoiceLayer isn't running, the bar shows \"disconnected\" state.</p>"},{"location":"plans/2026-02-24-flow-bar-design/#socket-protocol-v1","title":"Socket Protocol (v1)","text":""},{"location":"plans/2026-02-24-flow-bar-design/#events-voicelayer-bar","title":"Events (VoiceLayer \u2192 Bar)","text":"<pre><code>{\"type\": \"state\", \"state\": \"idle\"}\n{\"type\": \"state\", \"state\": \"speaking\", \"text\": \"What do you think about...\", \"voice\": \"jenny\"}\n{\"type\": \"state\", \"state\": \"recording\", \"mode\": \"vad\", \"silence_mode\": \"quick\"}\n{\"type\": \"state\", \"state\": \"recording\", \"mode\": \"ptt\"}\n{\"type\": \"state\", \"state\": \"transcribing\"}\n\n{\"type\": \"speech\", \"detected\": true}\n{\"type\": \"speech\", \"detected\": false}\n\n{\"type\": \"transcription\", \"text\": \"The user said this\"}\n\n{\"type\": \"error\", \"message\": \"Mic not available\", \"recoverable\": true}\n{\"type\": \"error\", \"message\": \"STT backend failed\", \"recoverable\": false}\n</code></pre>"},{"location":"plans/2026-02-24-flow-bar-design/#commands-bar-voicelayer","title":"Commands (Bar \u2192 VoiceLayer)","text":"<pre><code>{\"cmd\": \"stop\"}\n{\"cmd\": \"replay\"}\n{\"cmd\": \"toggle\", \"scope\": \"all\", \"enabled\": false}\n{\"cmd\": \"toggle\", \"scope\": \"tts\", \"enabled\": false}\n{\"cmd\": \"toggle\", \"scope\": \"mic\", \"enabled\": false}\n</code></pre>"},{"location":"plans/2026-02-24-flow-bar-design/#state-machine","title":"State Machine","text":"<pre><code>idle \u2192 speaking \u2192 idle                              (voice_speak)\nidle \u2192 speaking \u2192 recording \u2192 transcribing \u2192 idle   (voice_ask, speech detected)\nidle \u2192 speaking \u2192 recording \u2192 idle                  (voice_ask, no speech / stop)\nany  \u2192 error \u2192 (previous state or idle)\n</code></pre>"},{"location":"plans/2026-02-24-flow-bar-design/#contracts","title":"Contracts","text":"<ul> <li><code>speaking \u2192 idle</code> fires when afplay process exits (TTS playback done)</li> <li><code>recording \u2192 transcribing</code> fires when rec stops and whisper/STT starts</li> <li><code>transcribing \u2192 idle</code> fires after transcription result is sent</li> <li><code>speech</code> events fire during <code>recording</code> state only, true/false as VAD processes chunks</li> <li>Toggle scope matches existing flag files: <code>all</code> | <code>tts</code> | <code>mic</code></li> <li>Per-user socket path deferred (single-user macOS for now)</li> </ul>"},{"location":"plans/2026-02-24-flow-bar-design/#swiftui-bar-design","title":"SwiftUI Bar Design","text":""},{"location":"plans/2026-02-24-flow-bar-design/#position-size","title":"Position &amp; Size","text":"<ul> <li>Position: Bottom of screen, 60% from left edge (20% right of center)</li> <li>Size: ~280x40pt pill with rounded corners (24pt radius)</li> <li>Window level: Always on top (<code>.floating</code> window level)</li> <li>Background: Translucent vibrancy material (<code>.ultraThinMaterial</code>)</li> <li>When idle: Semi-transparent, subtle presence</li> <li>When active: Full opacity with state-appropriate colors</li> </ul>"},{"location":"plans/2026-02-24-flow-bar-design/#visual-states","title":"Visual States","text":"<pre><code>IDLE:          [ \ud83c\udf99 VoiceLayer          ]   Muted gray, subtle\nSPEAKING:      [ \u25b6  ||||||||||||    \u25a0   ]   Blue (#4A90D9), animated bars, stop button\nRECORDING:     [ \ud83d\udd34 ||||||||||||    \u2713   ]   Red pulse (#E54D4D), live bars, finish button\nTRANSCRIBING:  [ \u27f3  Processing...      ]   Blue spinner, brief state\nERROR:         [ \u26a0  Mic not found   \u00d7  ]   Yellow (#E5A84D), auto-dismiss 3s\nDISCONNECTED:  [ \u25cb  Disconnected       ]   Dim gray, no controls\n</code></pre>"},{"location":"plans/2026-02-24-flow-bar-design/#controls-per-state","title":"Controls Per State","text":"State Left Center Right idle mic icon (gray) \"VoiceLayer\" \u2014 speaking play icon (blue) waveform bars (animated) stop (\u25a0) recording red dot (pulsing) waveform bars (animated) finish (\u2713) transcribing spinner \"Processing...\" \u2014 error warning icon error message dismiss (\u00d7) disconnected empty circle \"Disconnected\" \u2014"},{"location":"plans/2026-02-24-flow-bar-design/#interactions","title":"Interactions","text":"<ul> <li>Click stop during speaking \u2192 <code>{\"cmd\": \"stop\"}</code> \u2192 kills TTS playback</li> <li>Click finish during recording \u2192 <code>{\"cmd\": \"stop\"}</code> \u2192 ends recording, triggers transcription</li> <li>Click pill when idle \u2192 expand to show toggle controls (TTS on/off, mic on/off)</li> <li>Right-click \u2192 replay last message (<code>{\"cmd\": \"replay\"}</code>)</li> <li>Drag \u2192 reposition the bar (persist position in UserDefaults)</li> </ul>"},{"location":"plans/2026-02-24-flow-bar-design/#animations","title":"Animations","text":"<ul> <li>Waveform bars: 5-7 vertical bars that animate height based on <code>speech.detected</code> events. Idle shimmer when waiting, active bounce when speech detected.</li> <li>Recording pulse: Red dot with subtle scale pulse animation (1.0 \u2192 1.2 \u2192 1.0, 1.5s cycle)</li> <li>State transitions: 200ms crossfade between states</li> <li>Error: Slide in from bottom, auto-dismiss after 3s with fade out</li> </ul>"},{"location":"plans/2026-02-24-flow-bar-design/#voicelayer-changes-required","title":"VoiceLayer Changes Required","text":""},{"location":"plans/2026-02-24-flow-bar-design/#new-socket-server-srcsocket-serverts","title":"New: Socket Server (<code>src/socket-server.ts</code>)","text":"<ul> <li>Create Unix domain socket at <code>/tmp/voicelayer.sock</code> on MCP server startup</li> <li>Accept multiple client connections (bar + potential future clients)</li> <li>Parse incoming JSON commands, dispatch to existing handlers</li> <li>Clean up socket file on shutdown (SIGTERM/SIGINT)</li> </ul>"},{"location":"plans/2026-02-24-flow-bar-design/#modified-state-emission-points","title":"Modified: State Emission Points","text":"File Where Event <code>src/tts.ts</code> <code>speak()</code> start <code>state: speaking</code> <code>src/tts.ts</code> <code>playAudioNonBlocking()</code> process exit <code>state: idle</code> <code>src/input.ts</code> <code>recordToBuffer()</code> start <code>state: recording</code> <code>src/input.ts</code> VAD chunk loop <code>speech: detected</code> <code>src/input.ts</code> <code>recordToBuffer()</code> finish <code>state: transcribing</code> <code>src/input.ts</code> <code>waitForInput()</code> after transcribe <code>state: idle</code> + <code>transcription</code> <code>src/mcp-server.ts</code> error catches <code>error</code> event"},{"location":"plans/2026-02-24-flow-bar-design/#modified-command-handlers","title":"Modified: Command Handlers","text":"<p>Socket <code>stop</code> command \u2192 write <code>/tmp/voicelayer-stop</code> (reuses existing mechanism) Socket <code>replay</code> command \u2192 call <code>playAudioNonBlocking(getHistoryEntry(0))</code> directly Socket <code>toggle</code> command \u2192 call existing toggle logic (write/delete flag files)</p>"},{"location":"plans/2026-02-24-flow-bar-design/#swiftui-project-structure","title":"SwiftUI Project Structure","text":"<pre><code>flow-bar/\n\u251c\u2500\u2500 Package.swift              # SPM package definition\n\u251c\u2500\u2500 Sources/\n\u2502   \u251c\u2500\u2500 FlowBarApp.swift       # @main, NSApplication setup, floating window\n\u2502   \u251c\u2500\u2500 BarView.swift          # Main pill view with state-driven UI\n\u2502   \u251c\u2500\u2500 WaveformView.swift     # Animated vertical bars\n\u2502   \u251c\u2500\u2500 SocketClient.swift     # Unix socket connection + reconnect\n\u2502   \u251c\u2500\u2500 VoiceState.swift       # ObservableObject state model\n\u2502   \u2514\u2500\u2500 Theme.swift            # Colors, sizes, animation constants\n\u2514\u2500\u2500 Resources/\n    \u2514\u2500\u2500 Assets.xcassets         # App icon\n</code></pre>"},{"location":"plans/2026-02-24-flow-bar-design/#v1-scope-mvp","title":"v1 Scope (MVP)","text":"<ul> <li>Socket server in VoiceLayer (create, accept, emit events, receive commands)</li> <li>State events: idle, speaking, recording, transcribing</li> <li>Commands: stop, toggle</li> <li>SwiftUI bar: state colors, stop/finish button, idle label</li> <li>Basic waveform animation (shimmer, not audio-driven)</li> <li>Reconnection logic (retry every 2s)</li> <li>Error display (auto-dismiss)</li> </ul>"},{"location":"plans/2026-02-24-flow-bar-design/#v15-scope-live-dictation","title":"v1.5 Scope (Live Dictation)","text":"<p>Live transcription in the bar \u2014 words appear as you speak, not just at the end.</p> <p>STT pipeline change: Batch \u2192 streaming. - Current: <code>rec \u2192 WAV file \u2192 whisper-cli \u2192 full text</code> - New: <code>rec stdout \u2192 pipe to whisper-cli --stream \u2192 parse partial results in real-time</code></p> <p>New protocol events: <pre><code>{\"type\": \"transcription\", \"text\": \"The user\", \"partial\": true}\n{\"type\": \"transcription\", \"text\": \"The user said this\", \"partial\": true}\n{\"type\": \"transcription\", \"text\": \"The user said this thing\", \"partial\": false}\n</code></pre></p> <p>Bar changes: - Recording state shows live text scrolling below the waveform bars - Text animates in word-by-word as partials arrive - Final (partial=false) is what gets returned to Claude - Bar pill expands vertically to fit ~2 lines of text, then scrolls</p> <p>whisper.cpp streaming: - <code>whisper-cli --stream</code> reads from stdin, outputs partial transcripts - Processes in ~1-2 second windows with overlap - Latency: ~500ms for first words to appear - Needs <code>--print-realtime</code> flag for streaming output</p>"},{"location":"plans/2026-02-24-flow-bar-design/#v2-scope-later","title":"v2 Scope (Later)","text":"<ul> <li>Audio-level driven waveform (<code>{\"type\": \"audio_level\", \"rms\": 0.42}</code>)</li> <li>Pause/resume recording</li> <li>Replay controls (right-click menu with history)</li> <li>Draggable positioning with persistence</li> <li>Expanded idle view (toggles, status)</li> <li>Launch at login (Login Items)</li> <li>Context-aware STT post-processing (developer vocabulary)</li> </ul>"},{"location":"plans/2026-02-24-flow-bar-design/#development-approach","title":"Development Approach","text":"<p>Red-green-refactor TDD: 1. Socket server tests first (Bun-side) 2. Protocol serialization tests 3. State emission at each integration point 4. SwiftUI previews for each visual state 5. Integration test: MCP \u2192 socket \u2192 bar state change</p>"},{"location":"plans/flow-bar/","title":"Flow Bar \u2014 VoiceLayer Floating Widget","text":"<p>Free, open-source Wispr Flow alternative for Claude Code. Native macOS SwiftUI pill + Unix socket IPC to VoiceLayer MCP server.</p> <p>Design doc: ../2026-02-24-flow-bar-design.md Research: <code>docs.local/logs/research-{1..6}-*.md</code></p>"},{"location":"plans/flow-bar/#progress","title":"Progress","text":"# Phase Folder Status Branch PR 1 Socket Server (Bun) phase-1 done feature/phase-1-socket-server PR #20 merged 2 State Emission phase-2 done feature/phase-2-state-emission PR #21 merged 3 SwiftUI App Scaffold phase-3 done feature/phase-3-swiftui-scaffold PR #22 merged 4 Socket Client + State phase-4 done feature/phase-4-socket-client-state PR #23 (merged with Phase 3) 5 Waveform + Visual Polish phase-5 done feature/phase-5-waveform-polish PR #24 merged 6 Integration + CLI phase-6 done feature/phase-6-integration-cli PR #25 merged 6.5 UI Polish (rename \u2192 Voice Bar, reposition, follow mouse) \u2014 done fix/voice-bar-ui-polish PR #26 7 Live Dictation (v1.5) phase-7 pending"},{"location":"plans/flow-bar/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       Unix socket        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  VoiceLayer MCP     \u2502  \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502  Flow Bar        \u2502\n\u2502  (Bun/TypeScript)   \u2502  /tmp/voicelayer.sock     \u2502  (SwiftUI app)   \u2502\n\u2502                     \u2502                           \u2502                  \u2502\n\u2502  Creates socket     \u2502  JSON newline-delimited   \u2502  Reconnecting    \u2502\n\u2502  Sends state events \u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba    \u2502  client          \u2502\n\u2502  Receives commands  \u2502  \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2502  Sends commands  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"plans/flow-bar/#key-research-findings","title":"Key Research Findings","text":"# Topic Key Insight Source 1 whisper streaming No <code>--stream</code> flag on whisper-cli. Need whisper-server sidecar compiled from source. ~1.5-2s latency per 3s chunk. research-1 2 SwiftUI + socket NSPanel + <code>.nonactivatingPanel</code> for non-focus-stealing. <code>NWEndpoint.unix()</code> for socket client. <code>.accessory</code> activation policy. research-2 3 Bun socket <code>Bun.listen({ unix })</code> native API. Backpressure via drain handler. Coexists cleanly with MCP stdio. research-3 4 macOS audio sox DOES trigger orange dot. AVAudioEngine recommended for SwiftUI app (10-40x lower latency, proper permissions). research-4 5 Waveform TimelineView 60fps, golden-ratio phase offsets, 3 modes (idle/listening/speech). Complete Swift code ready. research-5 6 Wispr Flow Electron ~800MB RAM, cloud-hybrid ASR, screenshot-based context, $8/mo. Native Swift beats it on resources. research-6"},{"location":"plans/flow-bar/#execution-rules","title":"Execution Rules","text":"<p>Each phase = one branch = one PR. TDD: write tests first, then implement.</p> <p>See <code>/large-plan</code> skill for the full protocol.</p>"},{"location":"plans/flow-bar/#cross-phase-knowledge","title":"Cross-Phase Knowledge","text":"<p>Update this section as phases complete: - Socket protocol spec \u2192 phase-1/findings.md - State machine transitions \u2192 phase-2/findings.md - SwiftUI window tricks \u2192 phase-3/findings.md - Waveform animation tuning \u2192 phase-5/findings.md</p>"},{"location":"plans/flow-bar/phase-1/","title":"Phase 1: Socket Server (Bun)","text":"<p>Back to main plan</p>"},{"location":"plans/flow-bar/phase-1/#goal","title":"Goal","text":"<p>Add a Unix domain socket server to VoiceLayer that broadcasts JSON state events and receives commands from the Flow Bar.</p>"},{"location":"plans/flow-bar/phase-1/#research-source","title":"Research Source","text":"<p><code>docs.local/logs/research-3-bun-unix-socket.md</code> \u2014 Complete <code>Bun.listen({ unix })</code> API reference with production-ready broadcast server code, backpressure handling, and MCP coexistence patterns.</p>"},{"location":"plans/flow-bar/phase-1/#key-technical-decisions","title":"Key Technical Decisions","text":"<ul> <li>API: <code>Bun.listen&lt;ClientData&gt;({ unix: SOCKET_PATH })</code> \u2014 native Bun, no <code>node:net</code> fallback needed</li> <li>Path: <code>/tmp/voicelayer.sock</code> (single-user macOS for now)</li> <li>Protocol: Newline-delimited JSON (NDJSON) \u2014 one JSON object per line, <code>\\n</code> terminated</li> <li>Backpressure: <code>socket.write()</code> returns bytes written, <code>-1</code> if dead. Queue in <code>pendingWrite</code>, flush in <code>drain</code> handler</li> <li>MCP coexistence: Socket server runs alongside StdioServerTransport on same event loop. All socket logging \u2192 <code>console.error()</code> (stderr only \u2014 stdout reserved for MCP JSON-RPC)</li> <li>Cleanup: Unlink socket file on startup (stale from crash) and on SIGINT/SIGTERM/exit</li> </ul>"},{"location":"plans/flow-bar/phase-1/#protocol-spec-v1","title":"Protocol Spec (v1)","text":""},{"location":"plans/flow-bar/phase-1/#events-voicelayer-bar","title":"Events (VoiceLayer \u2192 Bar)","text":"<pre><code>{\"type\": \"state\", \"state\": \"idle\"}\n{\"type\": \"state\", \"state\": \"speaking\", \"text\": \"What do you think about...\", \"voice\": \"jenny\"}\n{\"type\": \"state\", \"state\": \"recording\", \"mode\": \"vad\", \"silence_mode\": \"quick\"}\n{\"type\": \"state\", \"state\": \"recording\", \"mode\": \"ptt\"}\n{\"type\": \"state\", \"state\": \"transcribing\"}\n{\"type\": \"speech\", \"detected\": true}\n{\"type\": \"speech\", \"detected\": false}\n{\"type\": \"transcription\", \"text\": \"The user said this\"}\n{\"type\": \"error\", \"message\": \"Mic not available\", \"recoverable\": true}\n</code></pre>"},{"location":"plans/flow-bar/phase-1/#commands-bar-voicelayer","title":"Commands (Bar \u2192 VoiceLayer)","text":"<pre><code>{\"cmd\": \"stop\"}\n{\"cmd\": \"replay\"}\n{\"cmd\": \"toggle\", \"scope\": \"all\"|\"tts\"|\"mic\", \"enabled\": boolean}\n</code></pre>"},{"location":"plans/flow-bar/phase-1/#tools","title":"Tools","text":"<ul> <li>Code: Claude Code (Bun/TypeScript)</li> <li>Tests: <code>bun test</code></li> </ul>"},{"location":"plans/flow-bar/phase-1/#steps","title":"Steps","text":"<ol> <li>Define TypeScript types for socket protocol (events + commands) in <code>src/socket-protocol.ts</code></li> <li>Write tests for protocol serialization/deserialization</li> <li>Create <code>src/socket-server.ts</code> \u2014 <code>Bun.listen({ unix })</code> with client tracking, broadcast, backpressure, NDJSON framing</li> <li>Write tests for socket server: connect, receive broadcast, send command, disconnect handling, stale socket cleanup</li> <li>Create <code>broadcast()</code> and <code>handleCommand()</code> exports that other modules will call</li> <li>Wire socket server startup into <code>src/mcp-server.ts</code> \u2014 start alongside MCP, shut down on exit</li> <li>Add socket path to <code>src/paths.ts</code> constants</li> <li>Verify MCP + socket coexistence (both running, no stdout pollution)</li> <li>Update CLAUDE.md with socket server section</li> </ol>"},{"location":"plans/flow-bar/phase-1/#depends-on","title":"Depends On","text":"<ul> <li>None (first phase)</li> </ul>"},{"location":"plans/flow-bar/phase-1/#status","title":"Status","text":"<ul> <li>[ ] Protocol types (<code>src/socket-protocol.ts</code>)</li> <li>[ ] Protocol serialization tests</li> <li>[ ] Socket server (<code>src/socket-server.ts</code>)</li> <li>[ ] Socket server tests (connect, broadcast, command, disconnect, cleanup)</li> <li>[ ] Wire into MCP server startup</li> <li>[ ] Path constants in <code>src/paths.ts</code></li> <li>[ ] MCP coexistence verification</li> <li>[ ] CLAUDE.md update</li> </ul>"},{"location":"plans/flow-bar/phase-1/findings/","title":"Phase 1 Findings","text":""},{"location":"plans/flow-bar/phase-1/findings/#decisions","title":"Decisions","text":""},{"location":"plans/flow-bar/phase-1/findings/#research","title":"Research","text":""},{"location":"plans/flow-bar/phase-1/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/flow-bar/phase-1/findings/#notes","title":"Notes","text":""},{"location":"plans/flow-bar/phase-2/","title":"Phase 2: State Emission","text":"<p>Back to main plan</p>"},{"location":"plans/flow-bar/phase-2/#goal","title":"Goal","text":"<p>Wire state events into all VoiceLayer integration points so every voice action broadcasts its state to connected Flow Bar clients.</p>"},{"location":"plans/flow-bar/phase-2/#key-technical-decisions","title":"Key Technical Decisions","text":"<ul> <li>Emit points: Each state change calls <code>broadcast()</code> from <code>socket-server.ts</code></li> <li>State machine: <code>idle \u2192 speaking \u2192 idle</code> (voice_speak), <code>idle \u2192 speaking \u2192 recording \u2192 transcribing \u2192 idle</code> (voice_ask)</li> <li>Speech events: Fire during <code>recording</code> state only, <code>true</code>/<code>false</code> as VAD processes chunks</li> <li>Speaking end: Detected when afplay process exits (non-blocking TTS) or speak() returns (blocking)</li> <li>Lightweight: Just import <code>broadcast</code> and call it at the right points. No refactoring of existing code.</li> </ul>"},{"location":"plans/flow-bar/phase-2/#emission-points","title":"Emission Points","text":"File Location Event <code>src/tts.ts</code> <code>speak()</code> start <code>{\"type\": \"state\", \"state\": \"speaking\", \"text\": \"...\", \"voice\": \"...\"}</code> <code>src/tts.ts</code> <code>playAudioNonBlocking()</code> process exit <code>{\"type\": \"state\", \"state\": \"idle\"}</code> <code>src/input.ts</code> <code>recordToBuffer()</code> start <code>{\"type\": \"state\", \"state\": \"recording\", \"mode\": \"vad\"|\"ptt\"}</code> <code>src/input.ts</code> VAD chunk loop \u2014 speech detected <code>{\"type\": \"speech\", \"detected\": true}</code> <code>src/input.ts</code> VAD chunk loop \u2014 silence after speech <code>{\"type\": \"speech\", \"detected\": false}</code> <code>src/input.ts</code> <code>recordToBuffer()</code> finish <code>{\"type\": \"state\", \"state\": \"transcribing\"}</code> <code>src/stt.ts</code> or <code>src/input.ts</code> After transcription complete <code>{\"type\": \"state\", \"state\": \"idle\"}</code> + <code>{\"type\": \"transcription\", \"text\": \"...\"}</code> <code>src/mcp-server.ts</code> Error catches <code>{\"type\": \"error\", \"message\": \"...\", \"recoverable\": bool}</code>"},{"location":"plans/flow-bar/phase-2/#command-handlers","title":"Command Handlers","text":"Command Action <code>stop</code> Write <code>/tmp/voicelayer-stop</code> (reuses existing stop mechanism) + <code>pkill afplay</code> <code>replay</code> Call <code>playAudioNonBlocking(getHistoryEntry(0))</code> from tts.ts <code>toggle</code> Write/delete flag files (<code>/tmp/.claude_tts_disabled</code>, <code>/tmp/.claude_mic_disabled</code>)"},{"location":"plans/flow-bar/phase-2/#tools","title":"Tools","text":"<ul> <li>Code: Claude Code (Bun/TypeScript)</li> <li>Tests: <code>bun test</code></li> </ul>"},{"location":"plans/flow-bar/phase-2/#steps","title":"Steps","text":"<ol> <li>Read <code>src/tts.ts</code>, <code>src/input.ts</code>, <code>src/stt.ts</code>, <code>src/mcp-server.ts</code> to identify exact insertion points</li> <li>Write tests: mock broadcast, trigger voice_speak \u2192 assert speaking/idle events emitted</li> <li>Write tests: mock broadcast, trigger voice_ask \u2192 assert speaking/recording/speech/transcribing/idle events</li> <li>Add <code>broadcast()</code> calls to <code>src/tts.ts</code> \u2014 speaking start + idle on playback end</li> <li>Add <code>broadcast()</code> calls to <code>src/input.ts</code> \u2014 recording start + speech detected + transcribing + idle</li> <li>Add error event broadcasts to <code>src/mcp-server.ts</code> error handlers</li> <li>Implement command handlers in <code>socket-server.ts</code> \u2192 <code>handleCommand()</code>: stop, replay, toggle</li> <li>Write tests for command handlers (stop creates file, replay plays audio, toggle writes flags)</li> <li>Verify existing 116 tests still pass (no regressions from broadcast calls)</li> </ol>"},{"location":"plans/flow-bar/phase-2/#depends-on","title":"Depends On","text":"<ul> <li>Phase 1 (socket server must exist for <code>broadcast()</code> import)</li> </ul>"},{"location":"plans/flow-bar/phase-2/#status","title":"Status","text":"<ul> <li>[ ] Read and map all emission points in existing code</li> <li>[ ] Tests for speak event emission</li> <li>[ ] Tests for voice_ask event sequence</li> <li>[ ] Broadcast calls in <code>tts.ts</code></li> <li>[ ] Broadcast calls in <code>input.ts</code></li> <li>[ ] Error broadcasts in <code>mcp-server.ts</code></li> <li>[ ] Command handlers (stop, replay, toggle)</li> <li>[ ] Command handler tests</li> <li>[ ] Regression check (all existing tests pass)</li> </ul>"},{"location":"plans/flow-bar/phase-2/findings/","title":"Phase 2 Findings","text":""},{"location":"plans/flow-bar/phase-2/findings/#decisions","title":"Decisions","text":""},{"location":"plans/flow-bar/phase-2/findings/#research","title":"Research","text":""},{"location":"plans/flow-bar/phase-2/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/flow-bar/phase-2/findings/#notes","title":"Notes","text":""},{"location":"plans/flow-bar/phase-3/","title":"Phase 3: SwiftUI App Scaffold","text":"<p>Back to main plan</p>"},{"location":"plans/flow-bar/phase-3/#goal","title":"Goal","text":"<p>Create the Flow Bar SwiftUI macOS app \u2014 a frameless floating pill at the bottom of the screen with vibrancy, menu bar icon, and no dock icon.</p>"},{"location":"plans/flow-bar/phase-3/#research-sources","title":"Research Sources","text":"<ul> <li><code>docs.local/logs/research-2-swiftui-window-socket.md</code> \u2014 Complete NSPanel + floating window code</li> <li><code>docs.local/logs/research-5-waveform.swift</code> \u2014 WaveformView animation code</li> </ul>"},{"location":"plans/flow-bar/phase-3/#key-technical-decisions","title":"Key Technical Decisions","text":"<ul> <li>Project: Swift Package Manager executable (<code>swift build</code>, no Xcode project)</li> <li>macOS target: 14+ (required for <code>@Observable</code>, <code>spring(duration:bounce:)</code>)</li> <li>Window: <code>NSPanel</code> subclass with <code>.nonactivatingPanel</code> style mask (MUST be in init styleMask \u2014 known bug FB16484811)</li> <li>No dock icon: <code>.accessory</code> activation policy set in <code>applicationDidFinishLaunching</code></li> <li>Vibrancy: <code>NSVisualEffectView</code> wrapper (<code>.hudWindow</code> material, <code>.behindWindow</code> blending, state forced <code>.active</code>) \u2014 more reliable than SwiftUI <code>.ultraThinMaterial</code> in transparent panels</li> <li>Position: Bottom of screen, 60% from left edge (20% right of center)</li> <li>Size: 280x44pt pill, capsule-shaped (24pt corner radius)</li> <li>Not click-through: <code>ignoresMouseEvents = false</code>, <code>canBecomeKey = true</code> (for button hit-testing), but <code>.nonactivatingPanel</code> prevents focus steal</li> <li>Interaction: Buttons use <code>.buttonStyle(.plain)</code> which responds in non-key windows</li> </ul>"},{"location":"plans/flow-bar/phase-3/#project-structure","title":"Project Structure","text":"<pre><code>flow-bar/\n\u251c\u2500\u2500 Package.swift                 # SPM, macOS 14+\n\u251c\u2500\u2500 Sources/\n\u2502   \u2514\u2500\u2500 FlowBar/\n\u2502       \u251c\u2500\u2500 FlowBarApp.swift      # @main, AppDelegate, MenuBarExtra\n\u2502       \u251c\u2500\u2500 FloatingPanel.swift   # NSPanel subclass\n\u2502       \u251c\u2500\u2500 BarView.swift         # Main pill view\n\u2502       \u251c\u2500\u2500 WaveformView.swift    # Animated vertical bars\n\u2502       \u251c\u2500\u2500 VoiceState.swift      # @Observable state model\n\u2502       \u251c\u2500\u2500 SocketClient.swift    # (stub \u2014 wired in Phase 4)\n\u2502       \u2514\u2500\u2500 Theme.swift           # Colors, sizes, animation constants\n\u2514\u2500\u2500 mock_server.py                # Python test harness\n</code></pre>"},{"location":"plans/flow-bar/phase-3/#tools","title":"Tools","text":"<ul> <li>Code: Claude Code (Swift)</li> <li>Build: <code>swift build</code> / <code>swift run</code></li> <li>Preview: Xcode SwiftUI previews for visual states</li> </ul>"},{"location":"plans/flow-bar/phase-3/#steps","title":"Steps","text":"<ol> <li>Create <code>flow-bar/Package.swift</code> \u2014 SPM executable target, macOS 14+</li> <li>Create <code>FlowBarApp.swift</code> \u2014 <code>@main</code> App struct, <code>@NSApplicationDelegateAdaptor</code>, MenuBarExtra with connection status + quit</li> <li>Create <code>VoiceState.swift</code> \u2014 <code>@Observable</code> model with mode enum (idle, speaking, recording, transcribing, error, disconnected), transcript, isConnected, sendCommand closure</li> <li>Create <code>FloatingPanel.swift</code> \u2014 NSPanel subclass with <code>.nonactivatingPanel</code>, <code>.floating</code> level, transparent background, <code>canJoinAllSpaces</code>, <code>positionAtBottom()</code> at 60% from left</li> <li>Create <code>Theme.swift</code> \u2014 color constants (blue #4A90D9 speaking, red #E54D4D recording, gray #8E8E93 idle, yellow #E5A84D error), sizes, animation durations</li> <li>Create <code>BarView.swift</code> \u2014 main pill view with vibrancy background, state icon, status label, action buttons (stop/finish/toggle/replay), spring animations between states</li> <li>Create <code>WaveformView.swift</code> \u2014 7-bar waveform with TimelineView 60fps, idle shimmer / listening sway / speech bounce modes, golden-ratio phase offsets</li> <li>Create <code>SocketClient.swift</code> stub \u2014 empty class with <code>connect()</code>/<code>disconnect()</code>/<code>send()</code> (wired in Phase 4)</li> <li>Wire everything in AppDelegate: create VoiceState, panel, hosting view, position, show</li> <li>Create <code>mock_server.py</code> \u2014 Python Unix socket server that cycles through states for testing</li> <li>Verify: <code>swift build</code> succeeds, <code>swift run</code> shows pill at bottom, menu bar icon visible</li> <li>Test visual states by running mock server</li> </ol>"},{"location":"plans/flow-bar/phase-3/#depends-on","title":"Depends On","text":"<ul> <li>None (can develop in parallel with Phase 1-2, just needs mock server for testing)</li> </ul>"},{"location":"plans/flow-bar/phase-3/#status","title":"Status","text":"<ul> <li>[ ] Package.swift</li> <li>[ ] FlowBarApp.swift (entry point + MenuBarExtra)</li> <li>[ ] VoiceState.swift (@Observable model)</li> <li>[ ] FloatingPanel.swift (NSPanel subclass)</li> <li>[ ] Theme.swift (colors + sizes)</li> <li>[ ] BarView.swift (pill UI)</li> <li>[ ] WaveformView.swift (animated bars)</li> <li>[ ] SocketClient.swift (stub)</li> <li>[ ] AppDelegate wiring</li> <li>[ ] mock_server.py</li> <li>[ ] Build + run verification</li> <li>[ ] Visual state testing with mock server</li> </ul>"},{"location":"plans/flow-bar/phase-3/findings/","title":"Phase 3 Findings","text":""},{"location":"plans/flow-bar/phase-3/findings/#decisions","title":"Decisions","text":""},{"location":"plans/flow-bar/phase-3/findings/#research","title":"Research","text":""},{"location":"plans/flow-bar/phase-3/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/flow-bar/phase-3/findings/#notes","title":"Notes","text":""},{"location":"plans/flow-bar/phase-4/","title":"Phase 4: Socket Client + State Flow","text":"<p>Back to main plan</p>"},{"location":"plans/flow-bar/phase-4/#goal","title":"Goal","text":"<p>Implement the Unix domain socket client in Swift using Network.framework, connecting the Flow Bar to VoiceLayer's socket server with auto-reconnection and state updates.</p>"},{"location":"plans/flow-bar/phase-4/#research-source","title":"Research Source","text":"<p><code>docs.local/logs/research-2-swiftui-window-socket.md</code> \u2014 Complete <code>NWConnection</code> + <code>NWEndpoint.unix()</code> implementation with NDJSON framing and reconnection logic.</p>"},{"location":"plans/flow-bar/phase-4/#key-technical-decisions","title":"Key Technical Decisions","text":"<ul> <li>Framework: Network.framework (<code>NWConnection</code> + <code>NWEndpoint.unix(path:)</code>) \u2014 confirmed working on macOS 13+ by Apple DTS</li> <li>Transport: <code>.tcp</code> parameter provides stream-oriented semantics over Unix socket</li> <li>Framing: Buffer incoming data, split on <code>\\n</code>, parse each line as JSON (TCP delivers arbitrary chunks)</li> <li>Reconnection: Fixed 2-second delay, create NEW <code>NWConnection</code> each attempt (connections in <code>.failed</code>/<code>.cancelled</code> cannot restart)</li> <li>Thread safety: Socket callbacks on <code>DispatchQueue</code>, state updates dispatched to main thread via <code>DispatchQueue.main.async</code></li> <li>Command format: <code>{\"cmd\": \"stop\"}</code> \u2014 matches Phase 1 protocol spec</li> <li>Console noise: Expect harmless <code>nw_socket_set_common_sockopts</code> log \u2014 safe to ignore per Apple DTS</li> </ul>"},{"location":"plans/flow-bar/phase-4/#tools","title":"Tools","text":"<ul> <li>Code: Claude Code (Swift)</li> <li>Build: <code>swift build</code> / <code>swift run</code></li> <li>Test: Mock server (Python) from Phase 3</li> </ul>"},{"location":"plans/flow-bar/phase-4/#steps","title":"Steps","text":"<ol> <li>Implement <code>SocketClient.swift</code> \u2014 <code>NWConnection</code> to <code>/tmp/voicelayer.sock</code>, state update handler, receive loop</li> <li>Implement NDJSON receive buffer \u2014 accumulate chunks, split on <code>\\n</code>, parse JSON lines</li> <li>Implement <code>parseLine()</code> \u2014 map JSON fields to VoiceState properties (mode, transcript, error, speech detected)</li> <li>Implement <code>send(command:)</code> \u2014 serialize JSON command + <code>\\n</code>, send via connection</li> <li>Implement reconnection \u2014 on <code>.failed</code>/<code>.waiting</code>, cancel connection, wait 2s, create new <code>NWConnection</code></li> <li>Implement <code>disconnect()</code> \u2014 set <code>intentionallyClosed = true</code>, cancel connection, skip reconnect</li> <li>Wire into AppDelegate \u2014 create SocketClient, inject <code>sendCommand</code> closure into VoiceState, call <code>connect()</code> on launch, <code>disconnect()</code> on terminate</li> <li>Wire <code>VoiceState.stop()</code>/<code>toggle()</code>/<code>replay()</code> \u2192 <code>SocketClient.send()</code> via the closure</li> <li>Update <code>BarView.swift</code> \u2014 connect buttons to VoiceState actions, show disconnected state when <code>!isConnected</code></li> <li>Test with mock_server.py \u2014 verify state transitions, button commands, reconnection on server restart</li> <li>Test with actual VoiceLayer (Phase 1+2 must be merged) \u2014 full end-to-end</li> </ol>"},{"location":"plans/flow-bar/phase-4/#protocol-mapping-json-voicestate","title":"Protocol Mapping (JSON \u2192 VoiceState)","text":"JSON Field VoiceState Property <code>type: \"state\"</code>, <code>state: \"idle\"</code> <code>mode = .idle</code> <code>type: \"state\"</code>, <code>state: \"speaking\"</code>, <code>text: \"...\"</code> <code>mode = .speaking</code>, <code>responseText = text</code> <code>type: \"state\"</code>, <code>state: \"recording\"</code> <code>mode = .recording</code> <code>type: \"state\"</code>, <code>state: \"transcribing\"</code> <code>mode = .transcribing</code> <code>type: \"speech\"</code>, <code>detected: true/false</code> <code>speechDetected = detected</code> (drives waveform) <code>type: \"transcription\"</code>, <code>text: \"...\"</code> <code>transcript = text</code> <code>type: \"error\"</code>, <code>message: \"...\"</code> <code>mode = .error</code>, <code>errorMessage = message</code> Connection state <code>.ready</code> <code>isConnected = true</code> Connection <code>.failed</code>/<code>.cancelled</code> <code>isConnected = false</code>"},{"location":"plans/flow-bar/phase-4/#depends-on","title":"Depends On","text":"<ul> <li>Phase 3 (SwiftUI app must exist)</li> <li>Phase 1 (socket server for end-to-end testing \u2014 but mock_server.py works for development)</li> </ul>"},{"location":"plans/flow-bar/phase-4/#status","title":"Status","text":"<ul> <li>[ ] SocketClient NWConnection setup</li> <li>[ ] NDJSON receive buffer + line parsing</li> <li>[ ] JSON \u2192 VoiceState mapping</li> <li>[ ] Command sending</li> <li>[ ] Auto-reconnection (2s delay)</li> <li>[ ] Clean disconnect</li> <li>[ ] AppDelegate wiring</li> <li>[ ] Button \u2192 command wiring</li> <li>[ ] Disconnected state UI</li> <li>[ ] Mock server testing</li> <li>[ ] End-to-end testing with VoiceLayer</li> </ul>"},{"location":"plans/flow-bar/phase-4/findings/","title":"Phase 4 Findings","text":""},{"location":"plans/flow-bar/phase-4/findings/#decisions","title":"Decisions","text":""},{"location":"plans/flow-bar/phase-4/findings/#research","title":"Research","text":""},{"location":"plans/flow-bar/phase-4/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/flow-bar/phase-4/findings/#notes","title":"Notes","text":""},{"location":"plans/flow-bar/phase-5/","title":"Phase 5: Waveform + Visual Polish","text":"<p>Back to main plan</p>"},{"location":"plans/flow-bar/phase-5/#goal","title":"Goal","text":"<p>Integrate the hero waveform animation into the pill UI and polish all visual states to look incredible.</p>"},{"location":"plans/flow-bar/phase-5/#research-source","title":"Research Source","text":"<p><code>docs.local/logs/research-5-waveform.swift</code> \u2014 Complete WaveformView implementation with 3 modes, golden-ratio phase offsets, center-weighted bars, glow effects.</p>"},{"location":"plans/flow-bar/phase-5/#key-technical-decisions","title":"Key Technical Decisions","text":"<ul> <li>Animation: <code>TimelineView(.animation(minimumInterval: 1/60))</code> for 60fps</li> <li>Bar count: 7 bars, 6pt wide, 4pt spacing, max 28pt height, min 4pt</li> <li>Phase offsets: Golden ratio (<code>phi = 1.618...</code>) so bars never sync up</li> <li>Center weighting: Center bars taller (weight 1.0) vs edges (0.65) \u2014 natural arc</li> <li>Idle: Gentle breathing sine wave, barely visible (0.05 + 0.1 * breath)</li> <li>Listening/recording: Medium sway, waiting for speech</li> <li>Speech detected: Multi-frequency layers (fast 8.5Hz + medium 4.2Hz + slow 1.8Hz + pulse + jitter) \u2014 feels alive</li> <li>Colors: Red (#E54D4D) during recording, blue (#4A90D9) during speaking, gray (#8E8E93) idle</li> <li>Glow: Subtle shadow with mode-appropriate color and radius</li> <li>Transitions: <code>spring(duration: 0.3, bounce: 0.15)</code> between modes</li> </ul>"},{"location":"plans/flow-bar/phase-5/#visual-polish-targets","title":"Visual Polish Targets","text":"State Left Center Right Background idle mic icon (gray) \"VoiceLayer\" text \u2014 Dim vibrancy speaking play icon (blue) waveform (shimmer) stop button Full vibrancy, blue tint recording red dot (pulsing) waveform (active) finish button Full vibrancy, red border glow transcribing spinner \"Processing...\" \u2014 Full vibrancy error warning (yellow) error message dismiss Yellow border, auto-dismiss 3s disconnected empty circle \"Disconnected\" \u2014 Very dim"},{"location":"plans/flow-bar/phase-5/#tools","title":"Tools","text":"<ul> <li>Code: Claude Code (Swift)</li> <li>Preview: Xcode SwiftUI previews</li> </ul>"},{"location":"plans/flow-bar/phase-5/#steps","title":"Steps","text":"<ol> <li>Integrate <code>WaveformView.swift</code> into the project (adapt from research-5 code)</li> <li>Connect WaveformView to VoiceState \u2014 map speaking/recording/speechDetected to WaveformMode</li> <li>Update <code>BarView.swift</code> \u2014 replace placeholder center content with WaveformView during speaking/recording states</li> <li>Add recording pulse animation \u2014 red dot with scale pulse (1.0 \u2192 1.2 \u2192 1.0, 1.5s cycle)</li> <li>Add error auto-dismiss \u2014 3s timer, slide-in from bottom, fade out</li> <li>Add state transition animations \u2014 200ms crossfade between all states</li> <li>Polish idle state \u2014 subtle presence, \"VoiceLayer\" label, semi-transparent</li> <li>Polish speaking state \u2014 blue accent, animated waveform, prominent stop button</li> <li>Polish recording state \u2014 red accent, pulsing dot, active waveform, finish button</li> <li>Polish transcribing state \u2014 spinner animation, brief transitional state</li> <li>Add right-click context menu \u2014 replay last message</li> <li>Test all visual states with mock_server.py cycling through modes</li> <li>Screenshot/record each state for README documentation</li> </ol>"},{"location":"plans/flow-bar/phase-5/#depends-on","title":"Depends On","text":"<ul> <li>Phase 3 (SwiftUI app must exist)</li> <li>Phase 4 (socket client for state-driven animations)</li> </ul>"},{"location":"plans/flow-bar/phase-5/#status","title":"Status","text":"<ul> <li>[ ] WaveformView integration</li> <li>[ ] VoiceState \u2192 WaveformMode mapping</li> <li>[ ] BarView center content update</li> <li>[ ] Recording pulse animation</li> <li>[ ] Error auto-dismiss</li> <li>[ ] State transition animations</li> <li>[ ] Idle state polish</li> <li>[ ] Speaking state polish</li> <li>[ ] Recording state polish</li> <li>[ ] Transcribing state polish</li> <li>[ ] Right-click context menu</li> <li>[ ] Visual testing with mock server</li> <li>[ ] State screenshots</li> </ul>"},{"location":"plans/flow-bar/phase-5/findings/","title":"Phase 5 Findings","text":""},{"location":"plans/flow-bar/phase-5/findings/#decisions","title":"Decisions","text":""},{"location":"plans/flow-bar/phase-5/findings/#research","title":"Research","text":""},{"location":"plans/flow-bar/phase-5/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/flow-bar/phase-5/findings/#notes","title":"Notes","text":""},{"location":"plans/flow-bar/phase-6/","title":"Phase 6: Integration + CLI","text":"<p>Back to main plan</p>"},{"location":"plans/flow-bar/phase-6/#goal","title":"Goal","text":"<p>End-to-end integration testing, CLI command to launch the bar, and distribution as a local .app bundle.</p>"},{"location":"plans/flow-bar/phase-6/#key-technical-decisions","title":"Key Technical Decisions","text":"<ul> <li>CLI: <code>voicelayer bar</code> subcommand (added to <code>src/cli/voicelayer.sh</code>) \u2014 builds + launches the SwiftUI app</li> <li>Build: <code>swift build -c release</code> in the <code>flow-bar/</code> directory, copy binary to known location</li> <li>Auto-launch: Optional launchd plist for \"launch at login\" (similar to existing <code>com.golems.tts-daemon.plist</code>)</li> <li>.app bundle: Wrap the built binary in a minimal <code>.app</code> for proper icon/permissions attribution</li> <li>Info.plist: <code>LSUIElement = YES</code> for no dock icon, <code>NSMicrophoneUsageDescription</code> (future AVAudioEngine)</li> </ul>"},{"location":"plans/flow-bar/phase-6/#tools","title":"Tools","text":"<ul> <li>Code: Claude Code (Swift + Bash)</li> <li>Tests: Integration tests (Bun + Swift)</li> </ul>"},{"location":"plans/flow-bar/phase-6/#steps","title":"Steps","text":"<ol> <li>Write end-to-end integration test: start MCP server \u2192 socket server starts \u2192 launch mock bar \u2192 verify state events arrive</li> <li>Write command round-trip test: bar sends stop \u2192 VoiceLayer receives \u2192 stop file created</li> <li>Add <code>bar</code> subcommand to <code>src/cli/voicelayer.sh</code> \u2014 <code>swift build -c release</code> + launch binary</li> <li>Add <code>bar-stop</code> subcommand \u2014 find and kill the FlowBar process</li> <li>Create minimal <code>.app</code> bundle structure:    <pre><code>FlowBar.app/\n\u251c\u2500\u2500 Contents/\n\u2502   \u251c\u2500\u2500 Info.plist       # LSUIElement, NSMicrophoneUsageDescription\n\u2502   \u251c\u2500\u2500 MacOS/\n\u2502   \u2502   \u2514\u2500\u2500 FlowBar      # Built binary (symlink or copy)\n\u2502   \u2514\u2500\u2500 Resources/\n\u2502       \u2514\u2500\u2500 AppIcon.icns  # Icon\n</code></pre></li> <li>Create launchd plist for auto-launch: <code>com.voicelayer.flow-bar.plist</code></li> <li>Add <code>bar-install</code> subcommand \u2014 copy .app to ~/Applications, install launchd plist</li> <li>Test full workflow: <code>voicelayer bar</code> \u2192 pill appears \u2192 use voice_speak \u2192 bar shows speaking \u2192 stop button works</li> <li>Test reconnection: kill VoiceLayer \u2192 bar shows disconnected \u2192 restart \u2192 bar reconnects</li> <li>Update CLAUDE.md with Flow Bar section</li> <li>Update main README.md with Flow Bar quick start</li> </ol>"},{"location":"plans/flow-bar/phase-6/#depends-on","title":"Depends On","text":"<ul> <li>Phase 1-5 (everything must be working)</li> </ul>"},{"location":"plans/flow-bar/phase-6/#status","title":"Status","text":"<ul> <li>[ ] End-to-end integration test (state events)</li> <li>[ ] Command round-trip test</li> <li>[ ] <code>voicelayer bar</code> CLI command</li> <li>[ ] <code>voicelayer bar-stop</code> command</li> <li>[ ] .app bundle structure</li> <li>[ ] Launchd plist for auto-launch</li> <li>[ ] <code>voicelayer bar-install</code> command</li> <li>[ ] Full workflow test</li> <li>[ ] Reconnection test</li> <li>[ ] CLAUDE.md update</li> <li>[ ] README.md update</li> </ul>"},{"location":"plans/flow-bar/phase-6/findings/","title":"Phase 6 Findings","text":""},{"location":"plans/flow-bar/phase-6/findings/#decisions","title":"Decisions","text":""},{"location":"plans/flow-bar/phase-6/findings/#research","title":"Research","text":""},{"location":"plans/flow-bar/phase-6/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/flow-bar/phase-6/findings/#notes","title":"Notes","text":""},{"location":"plans/flow-bar/phase-7/","title":"Phase 7: Live Dictation (v1.5)","text":"<p>Back to main plan</p>"},{"location":"plans/flow-bar/phase-7/#goal","title":"Goal","text":"<p>Add streaming transcription so words appear in the Flow Bar as the user speaks, not just at the end.</p>"},{"location":"plans/flow-bar/phase-7/#research-sources","title":"Research Sources","text":"<ul> <li><code>docs.local/logs/research-1-whisper-streaming.md</code> \u2014 whisper-server sidecar, chunked HTTP POST, ~1.5-2s latency</li> <li><code>docs.local/logs/research-4-macos-audio-state.md</code> \u2014 AVAudioEngine for native audio capture (future consideration)</li> </ul>"},{"location":"plans/flow-bar/phase-7/#architecture-change","title":"Architecture Change","text":"<p>Current (batch): <pre><code>rec \u2192 WAV file \u2192 whisper-cli \u2192 full text (all at end)\n</code></pre></p> <p>New (streaming): <pre><code>rec stdout \u2192 Bun chunks (3s windows, 500ms overlap) \u2192 HTTP POST to whisper-server \u2192 partial JSON \u2192 socket \u2192 bar\n</code></pre></p>"},{"location":"plans/flow-bar/phase-7/#key-technical-decisions","title":"Key Technical Decisions","text":"<ul> <li>whisper-server: Compile from source (<code>cmake -DWHISPER_BUILD_SERVER=ON -DWHISPER_METAL=ON</code>). NOT included in Homebrew.</li> <li>Endpoint: <code>POST /inference</code> with multipart WAV, returns <code>{\"text\": \"...\"}</code>. One transcription at a time (mutex).</li> <li>Built-in VAD: whisper-server <code>--vad</code> flag uses Silero VAD in C++ \u2014 no JS-side VAD needed for dictation.</li> <li>Chunking: 3s chunks with 500ms overlap. AudioChunker ring buffer, PCM \u2192 WAV header, POST to server.</li> <li>Latency: ~1.5-2s per chunk on M1 Pro with large-v3-turbo. Good enough for dictation.</li> <li>Partial events: New protocol event <code>{\"type\": \"transcription\", \"text\": \"...\", \"partial\": true}</code> sent as each chunk completes.</li> <li>Final event: <code>{\"type\": \"transcription\", \"text\": \"...\", \"partial\": false}</code> when recording ends.</li> <li>Bar UI: Recording state expands pill vertically to show ~2 lines of live text, scrolling, word-by-word animation.</li> </ul>"},{"location":"plans/flow-bar/phase-7/#alternatives-evaluated","title":"Alternatives Evaluated","text":"Option Latency Quality Integration Verdict whisper-server + chunked POST ~1.5-2s Excellent HTTP fetch Chosen \u2014 best quality/integration balance WhisperLiveKit (mlx-whisper) ~3.3s Excellent WebSocket Higher latency, Python sidecar sherpa-onnx ~160ms Good Native npm Lower quality, but fastest whisper-stream (SDL2) ~1s Excellent Cannot pipe from Bun Not usable for our architecture"},{"location":"plans/flow-bar/phase-7/#setup-requirements","title":"Setup Requirements","text":"<pre><code># Build whisper-server from source\ngit clone https://github.com/ggml-org/whisper.cpp\ncd whisper.cpp\ncmake -B build -DWHISPER_METAL=ON -DWHISPER_SDL2=ON -DWHISPER_BUILD_SERVER=ON\ncmake --build build -j --config Release\n\n# Download VAD model for server-side filtering\n./models/download-vad-model.sh silero-v6.2.0\n</code></pre>"},{"location":"plans/flow-bar/phase-7/#tools","title":"Tools","text":"<ul> <li>Code: Claude Code (Bun/TypeScript + Swift)</li> <li>Tests: <code>bun test</code></li> </ul>"},{"location":"plans/flow-bar/phase-7/#steps","title":"Steps","text":"<ol> <li>Add <code>voicelayer whisper-server</code> CLI command \u2014 start/stop the sidecar process</li> <li>Create <code>src/streaming-stt.ts</code> \u2014 AudioChunker, PCM\u2192WAV encoder, HTTP POST to whisper-server <code>/inference</code></li> <li>Write tests for AudioChunker (correct chunk sizes, overlap, WAV header generation)</li> <li>Create <code>src/dictation.ts</code> \u2014 orchestrates rec \u2192 chunk \u2192 transcribe \u2192 emit partial events loop</li> <li>Wire partial transcription events into socket broadcast: <code>{\"type\": \"transcription\", \"text\": \"...\", \"partial\": true}</code></li> <li>Modify <code>src/input.ts</code> \u2014 add <code>streamingTranscribe</code> option that uses dictation pipeline instead of batch</li> <li>Update SwiftUI <code>VoiceState.swift</code> \u2014 add <code>partialTranscript</code> property, update on partial events</li> <li>Update SwiftUI <code>BarView.swift</code> \u2014 show live text in recording state, expand pill height, word-by-word animation</li> <li>Add whisper-server health check to MCP server startup (optional sidecar)</li> <li>Test with real speech \u2014 verify words appear within ~2s</li> <li>Update CLAUDE.md and README with streaming STT section</li> </ol>"},{"location":"plans/flow-bar/phase-7/#depends-on","title":"Depends On","text":"<ul> <li>Phase 1-6 (full v1 must be working)</li> <li>whisper-server compiled and accessible</li> </ul>"},{"location":"plans/flow-bar/phase-7/#status","title":"Status","text":"<ul> <li>[ ] <code>voicelayer whisper-server</code> CLI command</li> <li>[ ] AudioChunker + WAV encoder (<code>src/streaming-stt.ts</code>)</li> <li>[ ] AudioChunker tests</li> <li>[ ] Dictation orchestration (<code>src/dictation.ts</code>)</li> <li>[ ] Partial transcription socket events</li> <li>[ ] Streaming option in <code>input.ts</code></li> <li>[ ] VoiceState partialTranscript</li> <li>[ ] Live text UI in BarView</li> <li>[ ] whisper-server health check</li> <li>[ ] Real speech testing</li> <li>[ ] Docs update</li> </ul>"},{"location":"plans/flow-bar/phase-7/findings/","title":"Phase 7 Findings","text":""},{"location":"plans/flow-bar/phase-7/findings/#decisions","title":"Decisions","text":""},{"location":"plans/flow-bar/phase-7/findings/#research","title":"Research","text":""},{"location":"plans/flow-bar/phase-7/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/flow-bar/phase-7/findings/#notes","title":"Notes","text":""},{"location":"plans/voice-bar-v2/","title":"Voice Bar v2 \u2014 Full Wispr Flow Replacement","text":"<p>Replace Wispr Flow entirely: click-to-record, live dictation, expanding teleprompter, and native macOS polish.</p>"},{"location":"plans/voice-bar-v2/#context","title":"Context","text":"<p>Voice Bar v1 (PRs #20\u2013#27) delivered: - Floating dark pill with state indicators - Teleprompter word-by-word highlighting during TTS - Cancel/stop buttons, replay, waveform animations - Mouse tracking across screens, SwiftFormat CI</p> <p>v2 makes it a full Wispr Flow replacement \u2014 the user can unsubscribe from Wispr Flow after this.</p>"},{"location":"plans/voice-bar-v2/#progress","title":"Progress","text":"# Phase Folder Status PR Notes 1 Teleprompter Polish phase-1 done #28 Scroll anchor, replay, punctuation timing, wider view 2 Interactive Recording phase-2 done #29 Click-to-record, paste via CGEvent Cmd+V 3 Visual Polish &amp; UX phase-3 done #30 Expanding teleprompter, audio waveform, drag, idle collapse 4 Live Dictation phase-4 pending \u2014 Streaming STT \u2014 needs Etan present 5 Production Readiness phase-5 pending \u2014 Launch at login, center position, cleanup"},{"location":"plans/voice-bar-v2/#execution-rules","title":"Execution Rules","text":"<p>Each phase = one branch = one PR. See <code>/large-plan</code> skill for the full protocol.</p> <ol> <li>Branch: <code>feat/voice-bar-v2-phase-N</code></li> <li>PR loop: push \u2192 wait for Bugbot \u2192 fix comments \u2192 merge</li> <li>Never merge without checking review comments</li> <li>Tests must pass (<code>bun test</code>) + Swift build clean before push</li> <li>Update this table after each phase completes</li> </ol>"},{"location":"plans/voice-bar-v2/#dependencies","title":"Dependencies","text":"<ul> <li>Phase 4 requires Etan present (whisper-server compile + mic testing)</li> <li>Phase 4 booked: Wed Feb 25, 09:00\u201311:00</li> <li>All other phases can run autonomously</li> </ul>"},{"location":"plans/voice-bar-v2/#cross-phase-knowledge","title":"Cross-Phase Knowledge","text":"<p>Update this section as phases complete: - Design system: <code>memory/voice-bar-design-system.md</code> - Socket protocol: <code>src/socket-protocol.ts</code> (commands + events) - Theme tokens: <code>flow-bar/Sources/FlowBar/Theme.swift</code> - NSPanel quirks: FB16484811 (nonactivatingPanel must be in init styleMask)</p>"},{"location":"plans/voice-bar-v2/phase-1/","title":"Phase 1: Teleprompter Polish","text":"<p>Back to main plan</p>"},{"location":"plans/voice-bar-v2/phase-1/#goal","title":"Goal","text":"<p>Fix teleprompter word tracking, replay integration, and add expanding view for full text display.</p>"},{"location":"plans/voice-bar-v2/phase-1/#tools","title":"Tools","text":"<ul> <li>Code: Claude (Swift + TypeScript)</li> </ul>"},{"location":"plans/voice-bar-v2/phase-1/#steps","title":"Steps","text":"<ol> <li>Fix scroll anchor \u2014 current word should stay centered in view, not overflow left</li> <li>Change <code>ScrollViewReader</code> anchor from <code>.leading</code> to <code>.center</code></li> <li>Test with long sentences (10+ words)</li> <li>Replay re-triggers teleprompter \u2014 reset TeleprompterView when replay event fires</li> <li>VoiceState needs to re-emit speaking state on replay</li> <li>Or TeleprompterView resets when text changes to same value</li> <li>Expanding teleprompter view \u2014 pill expands vertically to show full text</li> <li>Smaller font (10pt instead of 12pt) for full text mode</li> <li>Current word highlighted bold white, upcoming dimmed, past words medium opacity</li> <li>Pill height animates from 44pt \u2192 ~70pt during speaking</li> <li>Panel height needs to accommodate expansion</li> <li>True word-timing estimation \u2014 improve timing model</li> <li>Account for punctuation pauses (commas, periods add delay)</li> <li>Adjust base rate to match edge-tts actual speech rate</li> <li>Tests \u2014 verify teleprompter word splitting, timing estimates</li> </ol>"},{"location":"plans/voice-bar-v2/phase-1/#depends-on","title":"Depends On","text":"<ul> <li>None</li> </ul>"},{"location":"plans/voice-bar-v2/phase-1/#status","title":"Status","text":"<ul> <li>[x] Fix scroll anchor to center (PR #28)</li> <li>[x] Replay re-triggers teleprompter (PR #28 \u2014 idle\u2192speaking broadcast)</li> <li>[ ] Expanding teleprompter view (deferred to Phase 3)</li> <li>[x] Improved word timing \u2014 punctuation pauses (PR #28)</li> <li>[ ] Tests (deferred \u2014 timing estimation is heuristic)</li> </ul>"},{"location":"plans/voice-bar-v2/phase-1/findings/","title":"Phase 1 Findings","text":""},{"location":"plans/voice-bar-v2/phase-1/findings/#decisions","title":"Decisions","text":"<ul> <li>[13:50] Scroll anchor <code>.center</code> instead of <code>.leading</code> \u2014 keeps current word visible</li> <li>[13:50] Replay fix goes in <code>mcp-server.ts</code> not Swift \u2014 broadcast speaking state before playing audio</li> <li>[13:50] Word timing: punctuation adds pauses (. ! ? = +150ms, , ; : = +80ms)</li> <li>[13:55] Expanding teleprompter: wider view (220pt) + smaller font (11pt) for Phase 1. Full vertical expansion deferred to Phase 3.</li> </ul>"},{"location":"plans/voice-bar-v2/phase-1/findings/#research","title":"Research","text":"<ul> <li>[13:45] Replay flow: <code>case \"replay\"</code> \u2192 <code>getHistoryEntry(0)</code> \u2192 <code>playAudioNonBlocking()</code>. Missing: no speaking broadcast before play. History entry has <code>.text</code> with original message.</li> <li>[13:45] Speaking broadcast truncates text to 200 chars (line 319 of tts.ts). Fine for teleprompter.</li> </ul>"},{"location":"plans/voice-bar-v2/phase-1/findings/#task-board","title":"Task Board","text":"Task Owner Status Fix scroll anchor to center claude done Replay broadcasts speaking state claude done Punctuation-aware word timing claude done Wider teleprompter (220pt, 11pt font) claude done Full vertical expansion \u2014 deferred to Phase 3"},{"location":"plans/voice-bar-v2/phase-1/findings/#notes","title":"Notes","text":"<ul> <li><code>playAudioNonBlocking()</code> already broadcasts idle when playback finishes (via <code>.exited</code> callback). So replay flow is: broadcast speaking \u2192 play audio \u2192 (auto) broadcast idle.</li> </ul>"},{"location":"plans/voice-bar-v2/phase-2/","title":"Phase 2: Interactive Recording","text":"<p>Back to main plan</p>"},{"location":"plans/voice-bar-v2/phase-2/#goal","title":"Goal","text":"<p>Make the Voice Bar a full input device \u2014 click to record, paste transcription on stop, like Wispr Flow.</p>"},{"location":"plans/voice-bar-v2/phase-2/#tools","title":"Tools","text":"<ul> <li>Research: Wispr Flow behavior study (how it pastes, where it focuses)</li> <li>Code: Claude (Swift + TypeScript)</li> </ul>"},{"location":"plans/voice-bar-v2/phase-2/#steps","title":"Steps","text":"<ol> <li>Click-to-record \u2014 tap the pill to start recording via socket command</li> <li>New socket command: <code>{\"cmd\": \"record\", \"mode\": \"vad\"}</code></li> <li>MCP server handles: start <code>voice_ask</code> flow programmatically</li> <li>Pill transitions to recording state with cancel (X) + stop (square)</li> <li>Paste transcription on stop \u2014 insert transcribed text at cursor position</li> <li>Use macOS Accessibility API or CGEvent to paste at current cursor</li> <li>Or use NSPasteboard + simulated Cmd+V</li> <li>Research: how Wispr Flow handles this (likely CGEvent key simulation)</li> <li>Handle recording lifecycle from bar</li> <li>Cancel (X) discards recording, returns to idle</li> <li>Stop (square) sends to STT, shows transcribing, then pastes result</li> <li>Transcription result event triggers paste</li> <li>Focus-aware recording \u2014 know which app was focused when recording started</li> <li>Store <code>NSWorkspace.shared.frontmostApplication</code> on record start</li> <li>Paste into that app (even if focus changed during recording)</li> <li>Visual feedback during paste \u2014 brief \"Pasted!\" confirmation in pill</li> </ol>"},{"location":"plans/voice-bar-v2/phase-2/#depends-on","title":"Depends On","text":"<ul> <li>None (but Phase 1 polish is nice-to-have first)</li> </ul>"},{"location":"plans/voice-bar-v2/phase-2/#status","title":"Status","text":"<ul> <li>[ ] Click-to-record command + handler</li> <li>[ ] Paste transcription via CGEvent/Accessibility</li> <li>[ ] Recording lifecycle (cancel/stop/transcribe/paste)</li> <li>[ ] Focus-aware paste target</li> <li>[ ] Visual paste confirmation</li> </ul>"},{"location":"plans/voice-bar-v2/phase-2/findings/","title":"Phase 2 Findings","text":""},{"location":"plans/voice-bar-v2/phase-2/findings/#decisions","title":"Decisions","text":""},{"location":"plans/voice-bar-v2/phase-2/findings/#research","title":"Research","text":""},{"location":"plans/voice-bar-v2/phase-2/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/voice-bar-v2/phase-2/findings/#notes","title":"Notes","text":""},{"location":"plans/voice-bar-v2/phase-3/","title":"Phase 3: Visual Polish &amp; UX","text":"<p>Back to main plan</p>"},{"location":"plans/voice-bar-v2/phase-3/#goal","title":"Goal","text":"<p>Polish the pill UX with audio-responsive waveform, idle collapse, draggable positioning, and smooth animations.</p>"},{"location":"plans/voice-bar-v2/phase-3/#tools","title":"Tools","text":"<ul> <li>Research: Wispr Flow UX patterns, macOS drag APIs</li> <li>Code: Claude (Swift)</li> </ul>"},{"location":"plans/voice-bar-v2/phase-3/#steps","title":"Steps","text":"<ol> <li>Expanding teleprompter \u2014 pill grows vertically during agent speech</li> <li>Pill height animates from 44pt \u2192 ~80pt when speaking with text</li> <li>Show 2-3 lines of text, current word highlighted, past lines fade</li> <li>Panel height must accommodate expansion</li> <li>Collapse back to 44pt when speaking ends</li> <li>Different from user dictation (Phase 4) \u2014 agent speech shows full readable text</li> <li>Audio-level driven waveform \u2014 replace shimmer with real RMS levels</li> <li>New socket event: <code>{\"type\": \"audio_level\", \"rms\": 0.42}</code></li> <li>Emit from <code>src/input.ts</code> during recording (already have RMS from VAD)</li> <li>Emit from <code>src/tts.ts</code> during speaking (estimate from audio analysis)</li> <li>WaveformView reads RMS to drive bar heights</li> <li>Idle collapse \u2014 pill shrinks after inactivity</li> <li>After 5s idle: collapse to small dot + green indicator only (~40pt wide)</li> <li>On hover: expand back to full pill smoothly</li> <li>On any state change: expand immediately</li> <li>Wispr Flow reference: collapses to ~30pt circle after inactivity</li> <li>Draggable pill position + persistence</li> <li><code>.gesture(DragGesture())</code> on the pill</li> <li>Save position to <code>UserDefaults</code> (x offset as percentage)</li> <li>Load on launch, fall back to <code>Theme.horizontalOffset</code></li> <li>Constraint: keep within screen bounds</li> <li>Smooth width transitions between states</li> <li>Currently <code>.fixedSize()</code> causes instant width changes</li> <li>Add <code>.animation(.smooth(duration: 0.3))</code> on the frame</li> <li>Test all state transitions for jank</li> <li>Haptic/sound feedback on button press</li> <li><code>NSHapticFeedbackManager</code> on stop/cancel/replay clicks</li> <li>Subtle, not annoying</li> </ol>"},{"location":"plans/voice-bar-v2/phase-3/#depends-on","title":"Depends On","text":"<ul> <li>None</li> </ul>"},{"location":"plans/voice-bar-v2/phase-3/#status","title":"Status","text":"<ul> <li>[x] Expanding teleprompter (vertical, 2-3 lines for agent speech)</li> <li>[x] Audio-level waveform (protocol + Swift + TS emission during recording)</li> <li>[x] Idle collapse with hover expand</li> <li>[x] Draggable positioning + UserDefaults persistence (via isMovableByWindowBackground + didMove observer)</li> <li>[x] Smooth width animation</li> <li>[x] Haptic feedback on buttons</li> </ul>"},{"location":"plans/voice-bar-v2/phase-3/findings/","title":"Phase 3 Findings","text":""},{"location":"plans/voice-bar-v2/phase-3/findings/#decisions","title":"Decisions","text":""},{"location":"plans/voice-bar-v2/phase-3/findings/#research","title":"Research","text":""},{"location":"plans/voice-bar-v2/phase-3/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/voice-bar-v2/phase-3/findings/#notes","title":"Notes","text":""},{"location":"plans/voice-bar-v2/phase-4/","title":"Phase 4: Live Dictation (Streaming STT)","text":"<p>Back to main plan</p>"},{"location":"plans/voice-bar-v2/phase-4/#goal","title":"Goal","text":"<p>Stream audio to whisper-server in real-time so words appear in the Voice Bar as the user speaks (1.5\u20132s latency).</p>"},{"location":"plans/voice-bar-v2/phase-4/#tools","title":"Tools","text":"<ul> <li>Research: whisper.cpp server API, streaming audio chunking strategies</li> <li>Code: Claude (TypeScript + Swift)</li> <li>Blocker: Needs Etan present for whisper-server compile + mic testing</li> </ul>"},{"location":"plans/voice-bar-v2/phase-4/#prerequisites-before-session","title":"Prerequisites (before session)","text":"<pre><code># Must compile whisper-server from source (NOT in Homebrew)\ngit clone https://github.com/ggml-org/whisper.cpp\ncd whisper.cpp\ncmake -B build -DWHISPER_METAL=ON -DWHISPER_SDL2=ON -DWHISPER_BUILD_SERVER=ON\ncmake --build build -j --config Release\n./models/download-vad-model.sh silero-v6.2.0\n</code></pre>"},{"location":"plans/voice-bar-v2/phase-4/#steps","title":"Steps","text":"<ol> <li><code>voicelayer whisper-server</code> CLI command \u2014 start/stop whisper-server sidecar</li> <li>Add to <code>src/cli/voicelayer.sh</code></li> <li>Health check endpoint: <code>GET /health</code></li> <li>Auto-start on MCP server startup if binary found</li> <li><code>src/streaming-stt.ts</code> \u2014 AudioChunker for streaming</li> <li>Pipe <code>rec</code> stdout through Bun in 3s windows with 500ms overlap</li> <li>PCM \u2192 WAV encoder (add WAV header to raw PCM chunks)</li> <li>HTTP POST each chunk to whisper-server <code>/inference</code></li> <li>Parse partial transcription from JSON response</li> <li><code>src/dictation.ts</code> \u2014 orchestrate recording \u2192 chunking \u2192 transcription loop</li> <li>Start <code>rec</code> to stdout (not file)</li> <li>Pipe through AudioChunker</li> <li>Each chunk \u2192 streaming-stt \u2192 partial text</li> <li>Broadcast partial transcription events via socket</li> <li>Socket protocol extension \u2014 partial transcription events</li> <li><code>{\"type\": \"transcription\", \"text\": \"...\", \"partial\": true}</code></li> <li><code>{\"type\": \"transcription\", \"text\": \"...\", \"partial\": false}</code> (final)</li> <li>VoiceState.swift \u2014 add <code>partialTranscript</code> property</li> <li>Update on partial events</li> <li>Clear on final event</li> <li>Show in pill during recording state</li> <li>Voice Bar recording UI \u2014 show live text as user speaks</li> <li>Expand pill to show partial transcription below waveform</li> <li>Words appear incrementally with animation</li> <li>Final transcription replaces partial</li> <li>Integration testing with real mic</li> <li>Latency acceptance testing (target: &lt; 2s)</li> <li>Accuracy testing across accents</li> <li>Background noise resilience</li> </ol>"},{"location":"plans/voice-bar-v2/phase-4/#depends-on","title":"Depends On","text":"<ul> <li>Phase 2 (recording lifecycle) \u2014 nice-to-have but not blocking</li> <li>Etan present for steps 1 (compile) and 7 (mic testing)</li> </ul>"},{"location":"plans/voice-bar-v2/phase-4/#status","title":"Status","text":"<ul> <li>[ ] whisper-server CLI command</li> <li>[ ] AudioChunker + streaming STT</li> <li>[ ] Dictation orchestrator</li> <li>[ ] Partial transcription protocol</li> <li>[ ] VoiceState partial transcript</li> <li>[ ] Live text in recording UI</li> <li>[ ] Integration testing with real mic</li> </ul>"},{"location":"plans/voice-bar-v2/phase-4/findings/","title":"Phase 4 Findings","text":""},{"location":"plans/voice-bar-v2/phase-4/findings/#decisions","title":"Decisions","text":""},{"location":"plans/voice-bar-v2/phase-4/findings/#research","title":"Research","text":""},{"location":"plans/voice-bar-v2/phase-4/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/voice-bar-v2/phase-4/findings/#notes","title":"Notes","text":""},{"location":"plans/voice-bar-v2/phase-5/","title":"Phase 5: Production Readiness","text":"<p>Back to main plan</p>"},{"location":"plans/voice-bar-v2/phase-5/#goal","title":"Goal","text":"<p>Ship-quality polish: launch at login, center positioning, context-aware STT, CLAUDE.md/README updates.</p>"},{"location":"plans/voice-bar-v2/phase-5/#tools","title":"Tools","text":"<ul> <li>Code: Claude (Swift + TypeScript + docs)</li> </ul>"},{"location":"plans/voice-bar-v2/phase-5/#steps","title":"Steps","text":"<ol> <li>Launch at login \u2014 Login Items integration</li> <li><code>voicelayer bar-install</code> CLI subcommand</li> <li>Register via <code>SMAppService.register()</code> (macOS 13+)</li> <li>Or generate launchd plist at <code>~/Library/LaunchAgents/com.voicelayer.bar.plist</code></li> <li><code>voicelayer bar-uninstall</code> to remove</li> <li>Move to center position (50%)</li> <li>Change <code>Theme.horizontalOffset</code> from 0.8 \u2192 0.5</li> <li>Only after Wispr Flow is uninstalled</li> <li>Make configurable via UserDefaults (ties into Phase 3 drag)</li> <li>Context-aware STT post-processing</li> <li>Developer vocabulary: common programming terms, framework names</li> <li>Code context: if focused app is terminal/IDE, boost code-related words</li> <li>API names from current project (scan package.json, imports)</li> <li>Post-process whisper output with vocabulary hints</li> <li>Update CLAUDE.md \u2014 document all new socket commands and events</li> <li>CancelCommand, RecordCommand (Phase 2)</li> <li>Partial transcription events (Phase 4)</li> <li>Audio level events (Phase 3)</li> <li>Voice Bar v2 architecture section</li> <li>Update README.md \u2014 user-facing documentation</li> <li>Installation instructions for Voice Bar</li> <li>Launch at login setup</li> <li>Click-to-record usage</li> <li>Configuration options</li> <li>npm publish \u2014 bump version, publish voicelayer-mcp with v2 features</li> <li>Update package.json version</li> <li>Update CHANGELOG</li> <li><code>npm publish</code></li> </ol>"},{"location":"plans/voice-bar-v2/phase-5/#depends-on","title":"Depends On","text":"<ul> <li>Phases 1\u20134 (this is the final polish phase)</li> </ul>"},{"location":"plans/voice-bar-v2/phase-5/#status","title":"Status","text":"<ul> <li>[ ] Launch at login</li> <li>[ ] Configurable position</li> <li>[ ] Context-aware STT</li> <li>[ ] CLAUDE.md updates</li> <li>[ ] README.md updates</li> <li>[ ] npm publish</li> </ul>"},{"location":"plans/voice-bar-v2/phase-5/findings/","title":"Phase 5 Findings","text":""},{"location":"plans/voice-bar-v2/phase-5/findings/#decisions","title":"Decisions","text":""},{"location":"plans/voice-bar-v2/phase-5/findings/#research","title":"Research","text":""},{"location":"plans/voice-bar-v2/phase-5/findings/#task-board","title":"Task Board","text":"Task Owner Status"},{"location":"plans/voice-bar-v2/phase-5/findings/#notes","title":"Notes","text":""},{"location":"plans/voicelayer-v3/","title":"VoiceLayer v3 \u2014 MCP Polish + Standalone Dictation","text":"<p>Perfect the MCP experience first, then build standalone dictation to replace Wispr Flow entirely.</p>"},{"location":"plans/voicelayer-v3/#baseline","title":"Baseline","text":"<ul> <li>Tests: 236 pass, 468 expect() calls</li> <li>PRs merged: #43 (word timestamps), #44 (paste fix), #45 (cleanup)</li> <li>Voice Bar: Dynamic sizing, karaoke sync, socket discovery, paste-on-record</li> <li>v2 phases done: 1-3 (teleprompter, recording, visual polish)</li> <li>v2 phases pending: 4 (live dictation), 5 (production readiness)</li> </ul>"},{"location":"plans/voicelayer-v3/#progress","title":"Progress","text":"# Phase Folder Status PR Checkpoint 1 MCP Sweep phase-1 DONE #47 MCP tools work flawlessly end-to-end 2 Voice Bar Hardening phase-2 in progress \u2014 Zero bugs in daily use for 1 week 3 Standalone Daemon phase-3 pending \u2014 <code>voicelayer serve</code> runs without Claude Code 4 Global Hotkeys phase-4 pending \u2014 Option+Space hold/double-tap, configurable 5 Batch STT Engine phase-5 pending \u2014 whisper small.en default, large-v3-turbo upgrade 6 Dev-Aware Post-Processing phase-6 pending \u2014 camelCase/snake_case commands, punctuation aliases 7 Integration + Production phase-7 pending \u2014 SMAppService, first-run setup, metrics, production build 8 LLM Correction (v2) phase-8 pending \u2014 Optional Llama 3.2 1B pass for ambiguity 9 Context-Aware Prompts (v2) phase-9 pending \u2014 Active app detection, IDE vs chat modes"},{"location":"plans/voicelayer-v3/#phase-details","title":"Phase Details","text":""},{"location":"plans/voicelayer-v3/#phase-1-mcp-sweep","title":"Phase 1: MCP Sweep","text":"<p>Goal: Every MCP tool (<code>voice_speak</code>, <code>voice_ask</code>) works perfectly. Fix edge cases, stale state, error recovery.</p> <ul> <li>[ ] Audit all MCP tool handlers \u2014 test each mode (announce, brief, consult, converse, think, replay, toggle)</li> <li>[ ] Fix any stale state bugs (e.g., stuck in \"recording\" after error)</li> <li>[ ] Verify <code>voice_ask</code> auto-waits for playback to finish</li> <li>[ ] Test TTS three-tier routing: Qwen3 -&gt; edge-tts -&gt; text-only</li> <li>[ ] Session booking: verify lock/unlock, stale lock cleanup</li> <li>[ ] Update MCP tool descriptions and parameter docs</li> <li>[ ] Test count target: maintain or increase from 236</li> </ul> <p>Depends on: nothing Audit: Run full test suite + manual MCP tool exercise</p>"},{"location":"plans/voicelayer-v3/#phase-2-voice-bar-hardening","title":"Phase 2: Voice Bar Hardening","text":"<p>Goal: Voice Bar is rock-solid for daily MCP use \u2014 no crashes, no disconnects, no visual glitches.</p> <ul> <li>[ ] Fix any remaining paste issues (test across iTerm2, VS Code, Chrome, Slack)</li> <li>[ ] Handle MCP server restart gracefully (discovery watcher + auto-reconnect verified)</li> <li>[ ] Voice Bar auto-launch: verify PR #42 works after fresh <code>npm install</code></li> <li>[ ] Error states: show meaningful messages, auto-recover to idle</li> <li>[ ] Memory/CPU profile: Voice Bar should idle at &lt;30MB, &lt;1% CPU</li> <li>[ ] Test collapsed pill: 5s idle -&gt; collapse -&gt; expand on any state change</li> <li>[ ] Verify dynamic panel sizing doesn't jitter on rapid state changes</li> </ul> <p>Depends on: Phase 1 Audit: 1 week of daily use, log errors, fix any issues found</p>"},{"location":"plans/voicelayer-v3/#phase-3-standalone-daemon","title":"Phase 3: Standalone Daemon","text":"<p>Goal: <code>voicelayer serve</code> starts the socket server + recording pipeline without MCP/Claude Code.</p> <ul> <li>[ ] New entry point: <code>src/daemon.ts</code> \u2014 starts socket server, handles commands, writes discovery file</li> <li>[ ] Reuse: <code>socket-server.ts</code>, <code>input.ts</code>, <code>stt.ts</code>, <code>vad.ts</code>, <code>session-booking.ts</code>, <code>paths.ts</code></li> <li>[ ] No MCP dependencies \u2014 no <code>@modelcontextprotocol/sdk</code> imports</li> <li>[ ] CLI command: <code>voicelayer serve [--port N]</code> (default: socket only, optional HTTP for health check)</li> <li>[ ] Graceful shutdown: SIGINT/SIGTERM -&gt; cleanup sockets, discovery file, stop recordings</li> <li>[ ] Coexistence: daemon detects if MCP server is already running (check discovery file PID)</li> <li>[ ] Voice Bar connects to daemon OR MCP server transparently (same socket protocol)</li> </ul> <p>Depends on: Phase 2 Research needed: None \u2014 architecture already proven standalone-capable Test: <code>voicelayer serve &amp;</code> -&gt; click Voice Bar -&gt; record -&gt; paste</p>"},{"location":"plans/voicelayer-v3/#phase-4-global-hotkeys-cgeventtap","title":"Phase 4: Global Hotkeys (CGEventTap)","text":"<p>Goal: Configurable hotkeys work system-wide, including fullscreen apps.</p> <ul> <li>[ ] CGEventTap in Voice Bar process (AppDelegate) \u2014 <code>.defaultTap</code> to consume hotkey</li> <li>[ ] Default: Globe (Fn) hold = push-to-talk, double-tap Globe = toggle, Ctrl+Shift+V = re-paste</li> <li>[ ] User-configurable hotkeys in <code>~/.voicelayer/config.yaml</code> (F5, Option+Space as alternatives)</li> <li>[ ] Double-tap detection (~300ms window)</li> <li>[ ] Input Monitoring permission check on startup (alongside Accessibility)</li> <li>[ ] Visual feedback: pill animates immediately on keypress (optimistic UI)</li> <li>[ ] Coexistence: when Claude MCP is active, hotkeys still work (daemon handles both)</li> </ul> <p>Depends on: Phase 3 Can parallelize with: Phase 5, Phase 6 (independent codebases) Research: Gemini research #1 covers CGEventTap patterns, key hold detection, double-tap timing Audit: Test in fullscreen apps, Electron apps, Terminal, web browsers</p>"},{"location":"plans/voicelayer-v3/#phase-5-batch-stt-engine","title":"Phase 5: Batch STT Engine","text":"<p>Goal: Fast, accurate local transcription using whisper large-v3-turbo.</p> <ul> <li>[ ] whisper.cpp with CoreML acceleration (ANE on Apple Silicon)</li> <li>[ ] Model management: auto-download large-v3-turbo on first use, progress UI</li> <li>[ ] First-run CoreML compilation (~15 min) with progress notification</li> <li>[ ] Batch transcription: record -&gt; stop -&gt; transcribe -&gt; paste. Target: &lt;1.5s for 10s clip</li> <li>[ ] Configurable model selection (small.en for speed, large-v3-turbo for accuracy)</li> <li>[ ] <code>--initial-prompt</code> with dev vocabulary (~224 tokens) for better code term recognition</li> <li>[ ] Benchmark: compare WER and latency vs current whisper-cli setup</li> </ul> <p>Depends on: Phase 3 Can parallelize with: Phase 4, Phase 6 (independent codebases) Research: Gemini research #1 (STT model comparison), Claude research (CoreML latency benchmarks)</p>"},{"location":"plans/voicelayer-v3/#phase-6-dev-aware-post-processing","title":"Phase 6: Dev-Aware Post-Processing","text":"<p>Goal: Transcribed text is developer-ready \u2014 proper casing, punctuation, code terms.</p> <ul> <li>[ ] Stage 1 \u2014 Rule-based pipeline (&lt;5ms):</li> <li>Spoken punctuation: ~50 mappings (\"open paren\" -&gt; <code>(</code>, \"semicolon\" -&gt; <code>;</code>, \"arrow\" -&gt; <code>=&gt;</code>)</li> <li>Case formatting commands: \"camel case foo bar\" -&gt; <code>fooBar</code>, \"snake case\" -&gt; <code>foo_bar</code></li> <li>Filler removal: \"um\", \"uh\", \"like\", \"you know\"</li> <li>Auto-capitalization after sentence boundaries</li> <li>Number formatting: \"forty two\" -&gt; <code>42</code></li> <li>[ ] Config file: <code>~/.voicelayer/aliases.yaml</code> for custom user aliases</li> <li>[ ] Tech vocabulary: ~500 common dev terms (\"use effect\" -&gt; <code>useEffect</code>, \"type script\" -&gt; <code>TypeScript</code>)</li> <li>[ ] Reference: Talon Voice + Serenade voice coding vocabularies</li> </ul> <p>Depends on: Phase 3 Can parallelize with: Phase 4, Phase 5 (pure text transforms, no Swift/STT deps) Research: Gemini research #2 (voice coding vocabularies, text expansion patterns)</p>"},{"location":"plans/voicelayer-v3/#phase-7-launch-at-login-production-build","title":"Phase 7: Launch at Login + Production Build","text":"<p>Goal: VoiceLayer starts automatically, runs as a proper macOS citizen.</p> <ul> <li>[ ] <code>SMAppService.mainApp.register()</code> for launch at login (macOS 13+)</li> <li>[ ] Proper code signing (Developer ID) so Accessibility/Input Monitoring permissions persist</li> <li>[ ] <code>LSUIElement = YES</code> in Info.plist (no Dock icon)</li> <li>[ ] First-run setup flow: permission prompts, model download, alias config</li> <li>[ ] Menu bar: status, preferences, quit</li> <li>[ ] Homebrew formula or npm global install: <code>npm i -g voicelayer</code></li> <li>[ ] Uninstall: <code>voicelayer uninstall</code> removes login item, caches, config</li> </ul> <p>Depends on: Phase 6 Research: Gemini research #1 (SMAppService vs LaunchAgent comparison)</p>"},{"location":"plans/voicelayer-v3/#phase-8-llm-correction-v2","title":"Phase 8: LLM Correction (v2)","text":"<p>Goal: Optional local LLM pass for ambiguous transcriptions.</p> <ul> <li>[ ] Llama 3.2 1B via MLX (Q4_K_M), ~400ms on M1 Pro</li> <li>[ ] User toggle: \"fast mode\" (rules only) vs \"smart mode\" (rules + LLM)</li> <li>[ ] LLM handles: context correction, \"no wait\" edits, ambiguous homophones</li> <li>[ ] Falls back to rules-only if LLM unavailable or slow</li> </ul> <p>Depends on: Phase 7 Defer until: v1 is shipping and daily-driven</p>"},{"location":"plans/voicelayer-v3/#phase-9-context-aware-prompts-v2","title":"Phase 9: Context-Aware Prompts (v2)","text":"<p>Goal: Adjust transcription based on active app.</p> <ul> <li>[ ] <code>NSWorkspace.frontmostApplication</code> -&gt; bundle ID detection</li> <li>[ ] IDE mode (VS Code, Xcode, Cursor): code-heavy initial prompt</li> <li>[ ] Chat mode (Slack, Telegram, iMessage): natural language prompt</li> <li>[ ] Terminal mode: command-heavy prompt</li> <li>[ ] Custom per-app profiles in <code>~/.voicelayer/contexts/</code></li> </ul> <p>Depends on: Phase 8 Defer until: v2</p>"},{"location":"plans/voicelayer-v3/#execution-rules","title":"Execution Rules","text":"<p>Same as v2: 1. Each phase = one branch = one PR 2. Branch naming: <code>feat/v3-phase-N-&lt;name&gt;</code> 3. Full PR loop: push -&gt; CI -&gt; review -&gt; triage -&gt; fix -&gt; merge 4. Tests must pass + Swift build clean before push 5. Update this table after each phase completes 6. Telegram notification at each checkpoint</p>"},{"location":"plans/voicelayer-v3/#quality-gates","title":"Quality Gates","text":"Gate When What MCP Perfect After Phase 2 All MCP tools work flawlessly, Voice Bar stable for 1 week Standalone MVP After Phase 5 Dictation works without Claude Code, F5 hotkeys, batch STT Wispr Replacement After Phase 7 Can quit Wispr Flow permanently, launch at login, dev formatting Full v2 After Phase 9 LLM correction, context-aware, production-grade"},{"location":"plans/voicelayer-v3/#cross-phase-knowledge","title":"Cross-Phase Knowledge","text":"<ul> <li>Socket protocol: <code>src/socket-protocol.ts</code></li> <li>Theme tokens: <code>flow-bar/Sources/FlowBar/Theme.swift</code></li> <li>MCP instructions: <code>src/mcp-tools.ts</code></li> <li>Voice Bar state: <code>flow-bar/Sources/FlowBar/VoiceState.swift</code></li> <li>Research: digested into BrainLayer (tag: <code>voicelayer standalone-dictation</code>)</li> <li>Gemini research #1: macOS APIs, global hotkeys, STT models, launch-at-login</li> <li>Gemini research #2: dev-aware transcription, voice coding, text expansion</li> <li>Claude research: architecture decisions, daemon design, v1 feature set</li> </ul>"},{"location":"use-cases/discovery-calls/","title":"Discovery Calls","text":"<p>Use VoiceLayer as a client discovery call assistant. The agent asks structured questions, tracks unknowns, detects red flags, and generates project briefs \u2014 all through voice interaction.</p>"},{"location":"use-cases/discovery-calls/#how-it-works","title":"How It Works","text":"<ol> <li>Agent greets the client via announce mode</li> <li>Agent asks discovery questions using converse mode</li> <li>While the client speaks, agent silently notes insights and flags via think mode</li> <li>Agent follows up on unknowns with targeted questions</li> <li>At the end, a structured project brief is generated</li> </ol>"},{"location":"use-cases/discovery-calls/#typical-session-flow","title":"Typical Session Flow","text":"<pre><code>Agent: voice_speak({ message: \"Starting discovery call for the e-commerce project\", mode: \"announce\" })\nAgent: voice_ask({ message: \"Can you walk me through the main goals for this project?\" })\nUser:  \"We need a Shopify replacement. Current store is too slow and we want custom checkout.\"\nAgent: voice_speak({ message: \"Shopify migration, performance + custom checkout are key drivers\", mode: \"think\", category: \"insight\" })\nAgent: voice_ask({ message: \"What's your timeline looking like?\" })\nUser:  \"We need it live by April, there's a big sale event\"\nAgent: voice_speak({ message: \"Hard deadline April - sale event. Tight timeline.\", mode: \"think\", category: \"red-flag\" })\nAgent: voice_ask({ message: \"How many products are in your current catalog?\" })\nUser:  \"Around 5000 SKUs with variants\"\nAgent: voice_speak({ message: \"5000 SKUs + variants = significant data migration\", mode: \"think\", category: \"insight\" })\nAgent: voice_speak({ message: \"Great conversation. Key takeaways: Shopify migration focused on performance and custom checkout, 5000 SKUs to migrate, hard April deadline for a sale event.\", mode: \"brief\" })\n</code></pre>"},{"location":"use-cases/discovery-calls/#discovery-categories","title":"Discovery Categories","text":"<p>VoiceLayer includes predefined discovery schemas covering 7 categories and 23 questions:</p> Category Focus Example Questions Goals Business objectives What problem are you solving? Users Target audience Who are the primary users? Technical Stack and constraints What's the current tech stack? Timeline Deadlines and phases When does this need to be live? Budget Resources and constraints What's the budget range? Competition Market position Who are your competitors? Success Metrics and KPIs How will you measure success?"},{"location":"use-cases/discovery-calls/#project-brief-generation","title":"Project Brief Generation","text":"<p>After a discovery call, findings are compiled into a project brief:</p> <pre><code># Project Brief: E-Commerce Migration\nDate: 2026-02-21\nClient: Example Corp\n\n## Overview\nMigration from Shopify to custom e-commerce platform.\nFocus: performance optimization and custom checkout flow.\n\n## Key Requirements\n- Custom checkout with multi-step flow\n- 5000+ SKU catalog migration with variants\n- Performance: sub-2s page loads\n\n## Red Flags\n- Hard April deadline (sale event) \u2014 tight for scope\n- No existing API documentation for current Shopify customizations\n\n## Unknowns\n- Payment processor preference (Stripe? PayPal?)\n- Hosting requirements (cloud provider, region)\n\n## Suggested Next Steps\n1. Technical audit of current Shopify store\n2. Data migration proof-of-concept (100 SKUs)\n3. Checkout flow wireframes for approval\n</code></pre> <p>Briefs are saved to <code>~/.voicelayer/briefs/</code> by default.</p>"},{"location":"use-cases/discovery-calls/#live-thinking-log","title":"Live Thinking Log","text":"<p>During the call, the agent's think notes are written to <code>/tmp/voicelayer-thinking.md</code>. Open this in a split-screen editor to see real-time insights:</p> <pre><code># In another terminal, watch the thinking log\ntail -f /tmp/voicelayer-thinking.md\n</code></pre> <p>This lets you glance at the agent's observations without interrupting the conversation.</p>"},{"location":"use-cases/discovery-calls/#tips","title":"Tips","text":"<ul> <li>Don't rush \u2014 set longer <code>timeout_seconds</code> (600+) for discovery calls</li> <li>Use think for red flags \u2014 <code>voice_speak({ message: \"...\", mode: \"think\", category: \"red-flag\" })</code> makes concerns easy to find later</li> <li>Consult before ending \u2014 \"I think we've covered the main topics. Anything else?\" via <code>voice_speak({ message: \"...\", mode: \"consult\" })</code></li> <li>Review the thinking log after the call \u2014 it often catches things you missed</li> </ul>"},{"location":"use-cases/qa-testing/","title":"QA Testing","text":"<p>Use VoiceLayer with Playwright for hands-free website testing. The agent browses pages, asks you what you see, and captures your observations in structured checklists.</p>"},{"location":"use-cases/qa-testing/#how-it-works","title":"How It Works","text":"<ol> <li>Agent navigates to a page using Playwright MCP</li> <li>Agent asks voice questions: \"How does the hero section look?\"</li> <li>You speak your observations: \"The image is stretched on mobile\"</li> <li>Agent logs findings using think mode (silent notes)</li> <li>At the end, a structured markdown report is generated</li> </ol>"},{"location":"use-cases/qa-testing/#typical-session-flow","title":"Typical Session Flow","text":"<pre><code>Agent: voice_speak({ message: \"Starting QA session for the checkout page\", mode: \"announce\" })\nAgent: [navigates to /checkout with Playwright]\nAgent: voice_ask({ message: \"How does the form layout look?\" })\nUser:  \"The credit card fields are misaligned, the expiry date wraps to a second line\"\nAgent: voice_speak({ message: \"CC fields misaligned, expiry wraps\", mode: \"think\", category: \"red-flag\" })\nAgent: voice_ask({ message: \"What about the submit button?\" })\nUser:  \"Looks good, centered, correct color\"\nAgent: voice_speak({ message: \"Submit button: PASS\", mode: \"think\", category: \"checklist-update\" })\nAgent: voice_speak({ message: \"Session complete. Found 1 issue: credit card field alignment on the checkout form.\", mode: \"brief\" })\n</code></pre>"},{"location":"use-cases/qa-testing/#qa-categories","title":"QA Categories","text":"<p>VoiceLayer includes predefined QA schemas covering 6 categories and 31 checks:</p> Category Checks Examples Layout Responsive, alignment, spacing Grid breaks, overflow, z-index Navigation Links, menus, breadcrumbs Dead links, mobile nav, active states Forms Validation, labels, errors Required fields, error messages, tab order Content Text, images, media Typos, alt text, broken images Performance Load time, animations Slow renders, janky scrolling Accessibility Contrast, screen readers, focus Color contrast, ARIA labels, keyboard nav"},{"location":"use-cases/qa-testing/#report-generation","title":"Report Generation","text":"<p>After a QA session, findings are compiled into a structured markdown report:</p> <pre><code># QA Report: Checkout Page\nDate: 2026-02-21\nURL: https://example.com/checkout\n\n## Issues Found\n\n### HIGH: Credit card field alignment\n- **Category:** Layout\n- **Details:** Expiry date field wraps to second line on viewport &lt; 768px\n- **Suggested fix:** Reduce input width or stack vertically on mobile\n\n## Passed Checks\n- Submit button: centered, correct color\n- Form labels: all present and clear\n</code></pre> <p>Reports are saved to <code>~/.voicelayer/reports/</code> by default.</p>"},{"location":"use-cases/qa-testing/#agent-configuration","title":"Agent Configuration","text":"<p>To use QA testing, your Claude Code session needs both VoiceLayer and Playwright MCP servers:</p> <pre><code>{\n  \"mcpServers\": {\n    \"voicelayer\": {\n      \"command\": \"bunx\",\n      \"args\": [\"voicelayer-mcp\"]\n    },\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\"@anthropic-ai/mcp-playwright\"]\n    }\n  }\n}\n</code></pre>"},{"location":"use-cases/qa-testing/#tips","title":"Tips","text":"<ul> <li>Use think mode liberally \u2014 <code>voice_speak({ message: \"...\", mode: \"think\" })</code> for silent notes that don't interrupt the flow</li> <li>Start with announce \u2014 give context before diving into questions</li> <li>End with brief \u2014 summarize findings so the user has a verbal recap</li> <li>Keep questions specific \u2014 \"How does the form look?\" works better than \"Any issues?\"</li> </ul>"}]}